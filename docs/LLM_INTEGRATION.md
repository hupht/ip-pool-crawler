# LLM é›†æˆæŒ‡å—

**ç‰ˆæœ¬**ï¼š1.0  
**æ—¥æœŸ**ï¼š2026-02-12

---

## ğŸ“Œ æ¦‚è¿°

é€šç”¨åŠ¨æ€çˆ¬è™«æ”¯æŒå¯é€‰çš„ AI è¾…åŠ©åŠŸèƒ½ï¼Œå¯ä»¥åœ¨å¯å‘å¼æ£€æµ‹å¤±è´¥æ—¶è°ƒç”¨ LLM æ¥æ”¹è¿›ç²¾å‡†åº¦ã€‚æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•é…ç½®å’Œä½¿ç”¨ LLM åŠŸèƒ½ã€‚

---

## ğŸ¤– LLM çš„ä½œç”¨

LLM åœ¨ä»¥ä¸‹åœºæ™¯è¢«è°ƒç”¨ï¼š

1. **ä½ç½®ä¿¡åº¦æ•°æ®**ï¼šå¯å‘å¼æ£€æµ‹çš„ç½®ä¿¡åº¦ < é˜ˆå€¼
2. **æ— è¡¨æ ¼**ï¼šé¡µé¢æ— è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç»“æ„
3. **è§£æå¤±è´¥**ï¼šå®Œå…¨æ— æ³•æå–ä»»ä½•æ•°æ®
4. **ç”¨æˆ·è¯·æ±‚**ï¼šç”¨æˆ·æ˜¾å¼è¦æ±‚ AI æ£€æŸ¥

LLM å°†è¿”å›ç»“æ„åŒ–çš„ JSON ç»“æœï¼ŒåŒ…å«ï¼š
```json
{
  "proxies": [
    {
      "ip": "1.2.3.4",
      "port": 8080,
      "protocol": "http",
      "confidence": 0.95,
      "reasoning": "ä»è¡¨æ ¼çš„ç¬¬äºŒåˆ—æå–"
    }
  ]
}
```

---

## âš™ï¸ å¿«é€Ÿé…ç½®

### 1. å¯ç”¨ LLM åŠŸèƒ½

ç¼–è¾‘ [`.env`](../.env.example)ï¼š

```bash
USE_AI_FALLBACK=true
```

### 2. é…ç½® LLM æœåŠ¡

#### é€‰é¡¹ Aï¼šä½¿ç”¨ OpenAIï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=sk-your-actual-key-here
```

1. è·å– API Keyï¼šhttps://platform.openai.com/api-keys
2. å¤åˆ¶åˆ° `.env` ä¸­
3. ç¡®ä¿è´¦æˆ·æœ‰è¶³å¤Ÿçš„é…é¢

#### é€‰é¡¹ Bï¼šä½¿ç”¨ Azure OpenAI

```bash
LLM_BASE_URL=https://your-resource.openai.azure.com/
LLM_MODEL=your-deployment-name
LLM_API_KEY=your-azure-api-key
```

#### é€‰é¡¹ Cï¼šä½¿ç”¨æœ¬åœ° Ollamaï¼ˆå…è´¹ã€ç¦»çº¿ï¼‰

å…ˆåœ¨æœ¬åœ°è¿è¡Œ Ollamaï¼š
```bash
ollama pull llama2
ollama serve  # é»˜è®¤ç›‘å¬ http://localhost:11434
```

ç„¶åé…ç½®ï¼š
```bash
LLM_BASE_URL=http://localhost:11434/v1
LLM_MODEL=llama2
LLM_API_KEY=dummy-key-for-local  # Ollama ä¸éœ€è¦çœŸå® key
```

#### é€‰é¡¹ Dï¼šä½¿ç”¨å…¶ä»–å…¼å®¹ OpenAI çš„æœåŠ¡

ä»»ä½•å…¼å®¹ OpenAI API çš„æœåŠ¡éƒ½å¯ä»¥ä½¿ç”¨ã€‚ç¤ºä¾‹ï¼š

```bash
# vLLM
LLM_BASE_URL=http://localhost:8000/v1
LLM_MODEL=meta-llama/Llama-2-7b

# LM Studio
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=local-model
```

### 3. é…ç½®è§¦å‘æ¡ä»¶

```bash
# é€‰æ‹©ä½•æ—¶è°ƒç”¨ AI
AI_TRIGGER_ON_LOW_CONFIDENCE=true    # ä½ç½®ä¿¡åº¦æ—¶
AI_TRIGGER_ON_NO_TABLE=true          # æ— è¡¨æ ¼æ—¶
AI_TRIGGER_ON_FAILED_PARSE=true      # è§£æå¤±è´¥
AI_TRIGGER_ON_USER_REQUEST=true      # ç”¨æˆ·è¯·æ±‚
```

### 4. é…ç½®æˆæœ¬æ§åˆ¶ï¼ˆä»…é’ˆå¯¹ä»˜è´¹æœåŠ¡ï¼‰

```bash
# ç¼“å­˜è®¾ç½®ï¼ˆé™ä½æˆæœ¬ï¼‰
AI_CACHE_ENABLED=true
AI_CACHE_TTL_HOURS=24

# æˆæœ¬é™åˆ¶
AI_COST_LIMIT_USD=100  # å•ä»»åŠ¡æœ€å¤š $100
```

### 5. é…ç½®æç¤ºè¯ä¸ HTML æäº¤ç­–ç•¥ï¼ˆæ–°å¢ï¼‰

```bash
# è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
LLM_SYSTEM_PROMPT=ä½ æ˜¯èµ„æ·±ä»£ç†æ•°æ®æŠ½å–å™¨ã€‚ä»…è¾“å‡ºåˆæ³• JSONï¼Œä¸è¦è¾“å‡ºè§£é‡Šã€Markdown æˆ–é¢å¤–æ–‡æœ¬ã€‚

# è‡ªå®šä¹‰ç”¨æˆ·æç¤ºè¯æ¨¡æ¿ï¼ˆå¯ç”¨å ä½ç¬¦ï¼š{context_json}ã€{html_snippet}ã€{html}ï¼‰
LLM_USER_PROMPT_TEMPLATE=ä»»åŠ¡ï¼šä» HTML ä¸­æå–ä»£ç†åˆ—è¡¨ï¼Œå¹¶ä¸¥æ ¼è¿”å› JSONã€‚\nè§„åˆ™ï¼š\n1) ä»…æå–å…¬ç½‘ IPv4ï¼Œè¿‡æ»¤ç§ç½‘/ä¿ç•™åœ°å€ã€‚\n2) port å¿…é¡»æ˜¯ 1-65535 çš„æ•´æ•°ã€‚\n3) protocol ç»Ÿä¸€ä¸º http/https/socks4/socks5ï¼ŒæœªçŸ¥æ—¶ç”¨ httpã€‚\n4) confidence å–å€¼ 0-1ã€‚\n5) æŒ‰ ip+port+protocol å»é‡ã€‚\n6) è‹¥æœªæå–åˆ°ç»“æœï¼Œè¿”å› {"proxies":[]}ã€‚\nè¾“å‡ºè¦æ±‚ï¼šä»…è¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼ä¸º {"proxies":[{"ip":"...","port":8080,"protocol":"http","confidence":0.95}]}ã€‚\nä¸Šä¸‹æ–‡ï¼š{context_json}\nHTMLï¼š\n{html_snippet}

# æ˜¯å¦æäº¤å®Œæ•´ HTMLï¼ˆtrue=æäº¤å…¨éƒ¨é¡µé¢ï¼Œfalse=æäº¤ç‰‡æ®µï¼‰
LLM_SUBMIT_FULL_HTML=false

# ä»…åœ¨ LLM_SUBMIT_FULL_HTML=false æ—¶ç”Ÿæ•ˆ
LLM_HTML_SNIPPET_CHARS=5000
```

âš ï¸ è¯´æ˜ï¼šæäº¤ç»™ LLM çš„å­—ç¬¦è¶Šå°‘ï¼Œé€šå¸¸æ•ˆæœè¶Šå·®ï¼ˆä¸Šä¸‹æ–‡ä¸è¶³ï¼Œæ¼æå–æ¦‚ç‡æ›´é«˜ï¼‰ã€‚

---

## ğŸ’° æˆæœ¬ä¼°ç®—

### OpenAI æˆæœ¬è®¡ç®—

**gpt-4o-mini** æ˜¯ç›®å‰æœ€ä¾¿å®œçš„é€‰é¡¹ï¼š

| æ“ä½œ | è€—è´¹ Token | æˆæœ¬ |
|------|-----------|------|
| 1 ä¸ª HTML é¡µé¢ | ~12K input | $0.0018 |
| 1 ä¸ª LLM è°ƒç”¨ | ~1K output | $0.0006 |
| **å•é¡µæ€»æˆæœ¬** | - | **~$0.002** |
| 100 é¡µçˆ¬å– | - | **~$0.20** |
| 1000 é¡µçˆ¬å– | - | **~$2.00** |

**æˆæœ¬ä¼˜åŒ–**ï¼š
1. å‡å°‘ä¸å¿…è¦çš„ AI è°ƒç”¨ï¼ˆè°ƒæ•´è§¦å‘æ¡ä»¶ï¼‰
2. å¯ç”¨ç¼“å­˜ï¼ˆç›¸åŒé¡µé¢ä¸é‡å¤è¯·æ±‚ï¼‰
3. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ï¼ˆgpt-3.5-turboï¼‰
4. ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆOllama å…è´¹ï¼‰

### æœˆåº¦é¢„ç®—å‚è€ƒ

```
å‡è®¾æ¯å¤©çˆ¬å– 1000 ä¸ªæ–° URLï¼š

æƒ…å†µ 1ï¼šå®Œå…¨å¯ç”¨ AI
  - æ¯æ—¥æˆæœ¬ï¼š1000 é¡µ Ã— $0.002 = $2
  - æœˆåº¦æˆæœ¬ï¼š$2 Ã— 30 = $60

æƒ…å†µ 2ï¼šä»…åœ¨å¤±è´¥æ—¶ä½¿ç”¨ AIï¼ˆå¤±è´¥ç‡ 10%ï¼‰
  - æ¯æ—¥æˆæœ¬ï¼š100 é¡µ Ã— $0.002 = $0.2
  - æœˆåº¦æˆæœ¬ï¼š$0.2 Ã— 30 = $6

æƒ…å†µ 3ï¼šå¯ç”¨ç¼“å­˜ + ç²¾å‡†è§¦å‘ï¼ˆæœ‰æ•ˆåˆ©ç”¨ç‡ 5%ï¼‰
  - æ¯æ—¥æˆæœ¬ï¼š50 é¡µ Ã— $0.002 = $0.1
  - æœˆåº¦æˆæœ¬ï¼š$0.1 Ã— 30 = $3
```

---

## ğŸ§ª æµ‹è¯• LLM é…ç½®

### æµ‹è¯•è¿æ¥

```bash
# ä½¿ç”¨ Python æµ‹è¯•
python -c "
from crawler.llm_config import LLMConfig
config = LLMConfig.from_env()
print(f'âœ“ é…ç½®æœ‰æ•ˆ: {config.model} @ {config.base_url}')
"
```

### æµ‹è¯• API è°ƒç”¨

```bash
python -c "
from crawler.llm_caller import LLMCaller
from crawler.llm_config import LLMConfig

config = LLMConfig.from_env()
caller = LLMCaller(config)

# ç®€å•æµ‹è¯•
result = caller.call_llm_for_parsing(
    html='<html><body>1.2.3.4:8080</body></html>',
    context={}
)
print(f'æˆåŠŸ: {result}')
"
```

### æŸ¥çœ‹ LLM æˆæœ¬æ—¥å¿—

```bash
# æŸ¥è¯¢æˆæœ¬è®°å½•
python -c "
from crawler.runtime import load_settings
from crawler.storage import get_mysql_connection

settings = load_settings()
conn = get_mysql_connection(settings)
try:
  with conn.cursor() as cursor:
    cursor.execute('SELECT llm_model, total_tokens, cost_usd, call_status, created_at FROM llm_call_log ORDER BY created_at DESC LIMIT 10')
    for row in cursor.fetchall():
      print(row)
finally:
  conn.close()
"
```

---

## ğŸ” å®‰å…¨æœ€ä½³å®è·µ

### 1. ä¿æŠ¤ API Key

âŒ **ä¸è¦åš**ï¼š
```bash
# ä¸è¦æäº¤åˆ° Git
git add .env
git commit -m "Add API key"

# ä¸è¦æ”¾åœ¨ä»£ç æ³¨é‡Šä¸­
LLM_API_KEY = "sk-xxx"  # My key
```

âœ… **è¦è¿™æ ·åš**ï¼š
```bash
# ä½¿ç”¨ .gitignore
echo ".env" >> .gitignore
git add .gitignore
git commit -m "Add .env to gitignore"

# åœ¨ .env ä¸­é…ç½®
LLM_API_KEY=sk-your-key

# æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡
export LLM_API_KEY=sk-your-key
```

### 2. è½®æ¢ API Key

å®šæœŸæ›´æ¢ API Keyï¼ˆå»ºè®®æ¯ 3 ä¸ªæœˆï¼‰ï¼š

```bash
# 1. ä»æœåŠ¡å•†è·å–æ–° Key
# 2. æ›´æ–° .env
LLM_API_KEY=sk-new-key-here

# 3. åˆ é™¤æ—§ Key
# 4. é‡å¯åº”ç”¨
python cli.py crawl-custom https://example.com
```

### 3. ç›‘æ§æˆæœ¬

å®šæœŸæ£€æŸ¥æˆæœ¬æ—¥å¿—ï¼š

```bash
# æŸ¥çœ‹ä»Šå¤©çš„ AI æˆæœ¬
python -c "
from datetime import datetime
from crawler.runtime import load_settings
from crawler.storage import get_mysql_connection

settings = load_settings()
conn = get_mysql_connection(settings)
today = datetime.now().date()

try:
  with conn.cursor() as cursor:
    cursor.execute('''
      SELECT COALESCE(SUM(cost_usd), 0) as total_cost
      FROM llm_call_log
      WHERE DATE(created_at) = %s
    ''', (today,))
    logs = cursor.fetchone()
finally:
  conn.close()

print(f'ä»Šæ—¥ AI æˆæœ¬: \${logs[0] or 0:.4f}')
"
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šLLM è¿æ¥å¤±è´¥

**ç—‡çŠ¶**ï¼š`Error connecting to LLM at https://api.openai.com/v1`

**æ’æŸ¥æ­¥éª¤**ï¼š

```bash
# 1. æ£€æŸ¥ç½‘ç»œè¿æ¥
ping api.openai.com

# 2. æ£€æŸ¥ API Key
echo $LLM_API_KEY

# 3. æ£€æŸ¥ base URL
curl -H "Authorization: Bearer $LLM_API_KEY" \
     https://api.openai.com/v1/models

# 4. æ£€æŸ¥ .env é…ç½®
cat .env | grep LLM_
```

### é—®é¢˜ 2ï¼šAPI Rate Limit

**ç—‡çŠ¶**ï¼š`Rate limit exceeded. Please retry after 60 seconds`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°‘å¹¶å‘åº¦
SOURCE_WORKERS=1

# å¢åŠ é‡è¯•å»¶è¿Ÿ
RETRY_BACKOFF_SECONDS=10

# å‡å°‘ AI è§¦å‘æ¡ä»¶
AI_TRIGGER_ON_LOW_CONFIDENCE=false
AI_TRIGGER_ON_NO_TABLE=false
```

### é—®é¢˜ 3ï¼šæˆæœ¬å¢é•¿è¿‡å¿«

**ç—‡çŠ¶**ï¼šè¿‘æœŸ LLM æˆæœ¬æ˜æ˜¾ä¸Šå‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å…ˆå‡å°‘è§¦å‘æ¡ä»¶
AI_TRIGGER_ON_LOW_CONFIDENCE=false
AI_TRIGGER_ON_NO_TABLE=false

# æˆ–ç¦ç”¨ AI
USE_AI_FALLBACK=false

# æˆ–å¯ç”¨ç¼“å­˜
AI_CACHE_ENABLED=true
```

### é—®é¢˜ 4ï¼šJSON è§£æé”™è¯¯

**ç—‡çŠ¶**ï¼š`Invalid JSON response from LLM`

**å¯èƒ½åŸå› **ï¼š
- æ¨¡å‹è¿”å›äº†é JSON æ ¼å¼
- æç¤ºè¯ä¸æ¸…æ¥š
- æ¨¡å‹ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# åˆ‡æ¢åˆ°æ›´å¯é çš„æ¨¡å‹
LLM_MODEL=gpt-4o-mini  # æ¨è

# ç¦ç”¨å¹¶é‡æ–°å¯ç”¨ AI
USE_AI_FALLBACK=false
# é‡å¯å
USE_AI_FALLBACK=true
```

---

## ğŸ“š é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æç¤ºè¯

æ¨èç›´æ¥åœ¨ `.env` é…ç½®ï¼Œæ— éœ€æ”¹æºç ï¼š

```bash
LLM_SYSTEM_PROMPT=ä½ æ˜¯èµ„æ·±ä»£ç†æ•°æ®æŠ½å–å™¨ã€‚ä»…è¾“å‡ºåˆæ³• JSONï¼Œä¸è¦è¾“å‡ºè§£é‡Šã€Markdown æˆ–é¢å¤–æ–‡æœ¬ã€‚
LLM_USER_PROMPT_TEMPLATE=ä»»åŠ¡ï¼šä» HTML ä¸­æå–ä»£ç†åˆ—è¡¨ï¼Œå¹¶ä¸¥æ ¼è¿”å› JSONã€‚\nè§„åˆ™ï¼š\n1) ä»…æå–å…¬ç½‘ IPv4ï¼Œè¿‡æ»¤ç§ç½‘/ä¿ç•™åœ°å€ã€‚\n2) port å¿…é¡»æ˜¯ 1-65535 çš„æ•´æ•°ã€‚\n3) protocol ç»Ÿä¸€ä¸º http/https/socks4/socks5ï¼ŒæœªçŸ¥æ—¶ç”¨ httpã€‚\n4) confidence å–å€¼ 0-1ã€‚\n5) æŒ‰ ip+port+protocol å»é‡ã€‚\n6) è‹¥æœªæå–åˆ°ç»“æœï¼Œè¿”å› {"proxies":[]}ã€‚\nè¾“å‡ºè¦æ±‚ï¼šä»…è¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼ä¸º {"proxies":[{"ip":"...","port":8080,"protocol":"http","confidence":0.95}]}ã€‚\nä¸Šä¸‹æ–‡ï¼š{context_json}\nHTMLï¼š\n{html_snippet}

# æäº¤ç­–ç•¥
LLM_SUBMIT_FULL_HTML=false
LLM_HTML_SNIPPET_CHARS=5000
```

âš ï¸ æç¤ºï¼šå½“ `LLM_SUBMIT_FULL_HTML=false` æ—¶ï¼Œæäº¤å­—ç¬¦è¶Šå°‘ï¼Œæå–æ•ˆæœé€šå¸¸è¶Šå·®ã€‚

### ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹

```bash
# ä½¿ç”¨ Anthropic Claude
LLM_BASE_URL=https://api.anthropic.com/v1
LLM_MODEL=claude-3-haiku
LLM_API_KEY=sk-ant-xxx

# ä½¿ç”¨ Google Gemini
LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta
LLM_MODEL=gemini-pro
LLM_API_KEY=your-gemini-key
```

### æ‰¹é‡æˆæœ¬ä¼°ç®—

```python
from crawler.llm_caller import estimate_batch_cost

# ä¼°ç®—çˆ¬å– 100 ä¸ª URL çš„æˆæœ¬
# å‡è®¾ 10% è°ƒç”¨ AI
total_cost = estimate_batch_cost(
    urls_count=100,
    ai_call_rate=0.1,
    model='gpt-4o-mini'
)
print(f"é¢„è®¡æˆæœ¬: ${total_cost:.2f}")
```

---

## ğŸ“ æ”¯æŒ

- **é—®é¢˜**ï¼šè§æœ¬æ–‡æ¡£ä¸‹æ–¹çš„æ•…éšœæ’æŸ¥
- **æ›´å¤šé…ç½®**ï¼šè§ [é…ç½®æŒ‡å—](./UNIVERSAL_CRAWLER_CONFIG.md)
- **ä½¿ç”¨ç¤ºä¾‹**ï¼šè§ [ä½¿ç”¨æŒ‡å—](./UNIVERSAL_CRAWLER_USAGE.md)
