# æ¨¡å—è¯¦ç»†æ–‡æ¡£

æœ¬æ–‡æ¡£æä¾›æ‰€æœ‰æ ¸å¿ƒæ¨¡å—çš„ API å‚è€ƒã€ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‘ ç›®å½•

### æ ¸å¿ƒçˆ¬è™«æ¨¡å—
1. [DynamicCrawler](#1-dynamiccrawler) - åŠ¨æ€çˆ¬è™«å¼•æ“
2. [UniversalParser](#2-universalparser) - é€šç”¨æ•°æ®è§£æå™¨
3. [StructureAnalyzer](#3-structureanalyzer) - HTML ç»“æ„åˆ†æ
4. [PaginationDetector](#4-paginationdetector) - åˆ†é¡µæ£€æµ‹
5. [PaginationController](#5-paginationcontroller) - åˆ†é¡µæ§åˆ¶

### AI æ¨¡å—
6. [LLMCaller](#6-llmcaller) - LLM API è°ƒç”¨
7. [LLMCache](#7-16-å…¶ä»–æ¨¡å—) - LLM ç»“æœç¼“å­˜
8. [LLMConfig](#7-16-å…¶ä»–æ¨¡å—) - LLM é…ç½®ç®¡ç†
9. [ErrorHandler](#7-16-å…¶ä»–æ¨¡å—) - æ™ºèƒ½é”™è¯¯å¤„ç†

### éªŒè¯æ¨¡å—
10. [ProxyValidator](#7-16-å…¶ä»–æ¨¡å—) - ä»£ç†éªŒè¯
11. [HTTPValidator](#7-16-å…¶ä»–æ¨¡å—) - HTTP éªŒè¯
12. [UniversalDetector](#7-16-å…¶ä»–æ¨¡å—) - æ¨¡å¼æ£€æµ‹

### ä¼ ç»Ÿæ¨¡å—
13. [Pipeline](#7-16-å…¶ä»–æ¨¡å—) - ä¼ ç»Ÿçˆ¬è™«æµæ°´çº¿
14. [Storage](#7-16-å…¶ä»–æ¨¡å—) - å­˜å‚¨å±‚
15. [Validator](#7-16-å…¶ä»–æ¨¡å—) - TCP éªŒè¯
16. [Checker](#7-16-å…¶ä»–æ¨¡å—) - å¤±è´¥çª—å£ç®¡ç†

---

## 1. DynamicCrawler

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.dynamic_crawler import DynamicCrawler, DynamicCrawlResult
```

### ğŸ“– ç±»å®šä¹‰

#### DynamicCrawler

**åˆå§‹åŒ–**ï¼š
```python
def __init__(self, settings: Settings):
    """
    å‚æ•°:
        settings: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ•°æ®åº“ã€LLMã€çˆ¬è™«ç­‰é…ç½®
    """
```

**ä¸»è¦æ–¹æ³•**ï¼š

##### crawl()
```python
def crawl(
    self,
    url: str,
    max_pages: int = 1,
    use_ai: bool = False,
    no_store: bool = False,
    verbose: bool = False,
    render_js: bool = False,
) -> DynamicCrawlResult:
    """
    æ‰§è¡ŒåŠ¨æ€çˆ¬å–
    
    å‚æ•°:
        url: èµ·å§‹URL
        max_pages: æœ€å¤§çˆ¬å–é¡µæ•° (1-100)
        use_ai: æ˜¯å¦å¯ç”¨AIè¾…åŠ©
        no_store: æ˜¯å¦åªæµ‹è¯•ä¸å­˜å‚¨
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        render_js: æ˜¯å¦å¯ç”¨ Playwright æ¸²æŸ“åè§£æ
        
    è¿”å›:
        DynamicCrawlResult å¯¹è±¡ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯
        
    å¼‚å¸¸:
        requests.RequestException: ç½‘ç»œè¯·æ±‚å¤±è´¥
        Exception: å…¶ä»–å¤„ç†é”™è¯¯
    """
```

**å…³é”®å†…éƒ¨èƒ½åŠ›ï¼ˆåŠ¨æ€æ¥å£åœºæ™¯ï¼‰**ï¼š
- `_discover_proxy_api_records(...)`
  - åœ¨ HTML ä¸è„šæœ¬ä¸­æå–å€™é€‰ API URL
  - æŒ‰ç™½åå•/é»‘åå•è¿‡æ»¤å¹¶æ¢æµ‹ JSON æ¥å£
- `_discover_runtime_api_records(...)`
  - ä½¿ç”¨ Playwright æ•è·è¿è¡Œæ—¶ `xhr/fetch` JSON å“åº”
  - é€‚é…ç­¾åæ¥å£ã€åŠ¨æ€ token åœºæ™¯
- `crawler.js_fetcher.fetch_page_and_api_payloads_with_playwright(...)`
  - åŒæ—¶è¿”å›æ¸²æŸ“å HTML ä¸æ•è·åˆ°çš„ JSON payload åˆ—è¡¨
  - æ”¯æŒ `max_payloads` ä¸ `max_response_bytes` é™åˆ¶

**è§¦å‘é¡ºåº**ï¼ˆç®€åŒ–ï¼‰ï¼š
1. å¸¸è§„ HTML è§£æ
2. æ¥å£è‡ªåŠ¨å‘ç°ï¼ˆ`API_DISCOVERY_*`ï¼‰
3. è¿è¡Œæ—¶ sniff å›é€€ï¼ˆ`RUNTIME_API_SNIFF_*`ï¼Œä¸”é `render_js` è·¯å¾„ï¼‰

#### DynamicCrawlResult

**æ•°æ®ç»“æ„**ï¼š
```python
@dataclass
class DynamicCrawlResult:
    url: str                      # èµ·å§‹URL
    pages_crawled: int            # çˆ¬å–é¡µæ•°
    extracted: int                # æå–æ€»æ•°
    valid: int                    # æœ‰æ•ˆæ•°
    invalid: int                  # æ— æ•ˆæ•°
    stored: int                   # å­˜å‚¨æ•°
    session_id: Optional[int]     # ä¼šè¯ID
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

#### åŸºç¡€ç”¨æ³•
```python
from crawler.runtime import load_settings
from crawler.dynamic_crawler import DynamicCrawler

# åŠ è½½é…ç½®
settings = load_settings(".env")

# åˆ›å»ºçˆ¬è™«
crawler = DynamicCrawler(settings)

# æ‰§è¡Œçˆ¬å–
result = crawler.crawl(
    url="https://example.com/proxy",
    max_pages=5,
    verbose=True
)

print(f"çˆ¬å– {result.pages_crawled} é¡µ")
print(f"æå– {result.extracted} æ¡")
print(f"æœ‰æ•ˆ {result.valid} æ¡")
print(f"å­˜å‚¨ {result.stored} æ¡")
```

#### AI è¾…åŠ©æ¨¡å¼
```python
result = crawler.crawl(
    url="https://complex-site.com/proxy",
    max_pages=3,
    use_ai=True,  # å¯ç”¨ AI
    verbose=True
)

if result.session_id:
    # æŸ¥è¯¢AIè°ƒç”¨æ—¥å¿—
    conn = get_mysql_connection(settings)
    with conn.cursor() as cur:
        cur.execute(
            "SELECT * FROM llm_call_logs WHERE session_id = %s",
            (result.session_id,)
        )
        logs = cur.fetchall()
        print(f"AIè°ƒç”¨æ¬¡æ•°: {len(logs)}")
```

#### æµ‹è¯•æ¨¡å¼
```python
result = crawler.crawl(
    url="https://unknown-site.com/proxy",
    max_pages=1,
    no_store=True,  # ä¸å­˜å‚¨ï¼Œåªæµ‹è¯•
    verbose=True
)

# æŸ¥çœ‹æå–ç»“æœï¼Œå†³å®šæ˜¯å¦æ­£å¼çˆ¬å–
if result.valid > 10:
    print("è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥æ­£å¼çˆ¬å–")
else:
    print("è´¨é‡è¾ƒå·®ï¼Œè€ƒè™‘å¯ç”¨AIæˆ–æ”¾å¼ƒ")
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **é¦–æ¬¡æµ‹è¯•**ï¼šä½¿ç”¨ `no_store=True` å’Œ `max_pages=1`
2. **è´¨é‡è¯„ä¼°**ï¼šæ£€æŸ¥ `valid / extracted` æ¯”ç‡
3. **AI ç­–ç•¥**ï¼šè´¨é‡å·®æ—¶å¯ç”¨ `use_ai=True`
4. **é¡µæ•°é™åˆ¶**ï¼šä¸ç¡®å®šæ—¶ä»å°å¼€å§‹ï¼ˆ3-5é¡µï¼‰
5. **åŠ¨æ€æ¥å£ç­–ç•¥**ï¼šä¼˜å…ˆå¼€å¯ `API_DISCOVERY_ENABLED`ï¼Œç­¾åç«™ç‚¹å†å¯ç”¨ `RUNTIME_API_SNIFF_ENABLED`

---

## 2. UniversalParser

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.universal_parser import UniversalParser, ProxyExtraction
```

### ğŸ“– ç±»å®šä¹‰

#### UniversalParser

**é™æ€æ–¹æ³•**ï¼š

##### parse()
```python
@staticmethod
def parse(
    html: Union[str, bytes],
    structure: Optional[Dict[str, Any]] = None,
    user_prompt: Optional[str] = None,
) -> List[ProxyExtraction]:
    """
    é€šç”¨è§£æHTML
    
    å‚æ•°:
        html: HTMLå†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–å­—èŠ‚ï¼‰
        structure: é¢„åˆ†æçš„ç»“æ„ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨è°ƒç”¨StructureAnalyzerï¼‰
        user_prompt: ç”¨æˆ·æç¤ºï¼ˆä¿ç•™ï¼Œæš‚æœªä½¿ç”¨ï¼‰
        
    è¿”å›:
        ProxyExtraction åˆ—è¡¨
    """
```

##### extract_all()
```python
@staticmethod
def extract_all(html: str) -> Tuple[List[ProxyExtraction], Dict[str, int]]:
    """
    å®Œæ•´æå–æµç¨‹ï¼ˆè§£æ + ç»Ÿè®¡ï¼‰
    
    è¿”å›:
        (æå–åˆ—è¡¨, ç»Ÿè®¡å­—å…¸)
        
    ç»Ÿè®¡å­—å…¸ç¤ºä¾‹:
        {
            "total": 100,
            "from_table": 80,
            "from_json": 10,
            "from_list": 5,
            "from_text": 5,
            "avg_confidence": 0.85
        }
    """
```

#### ProxyExtraction

**æ•°æ®ç»“æ„**ï¼š
```python
@dataclass
class ProxyExtraction:
    ip: str                               # IPåœ°å€
    port: Optional[int] = None            # ç«¯å£
    protocol: Optional[str] = None        # åè®®
    source_type: str = "unknown"          # æ¥æºç±»å‹
    confidence: float = 0.0               # ç½®ä¿¡åº¦
    raw_data: Optional[str] = None        # åŸå§‹æ•°æ®
    additional_info: Dict[str, Any] = field(default_factory=dict)
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

#### åŸºç¡€è§£æ
```python
from crawler.universal_parser import UniversalParser

html = """
<table>
  <tr><th>IP</th><th>Port</th><th>Protocol</th></tr>
  <tr><td>1.2.3.4</td><td>8080</td><td>HTTP</td></tr>
  <tr><td>5.6.7.8</td><td>3128</td><td>HTTPS</td></tr>
</table>
"""

extractions = UniversalParser.parse(html)

for ext in extractions:
    print(f"{ext.ip}:{ext.port} ({ext.protocol})")
    print(f"  ç½®ä¿¡åº¦: {ext.confidence:.2f}")
    print(f"  æ¥æº: {ext.source_type}")
```

#### å¸¦ç»Ÿè®¡ä¿¡æ¯
```python
extractions, stats = UniversalParser.extract_all(html)

print(f"æ€»è®¡: {stats['total']}")
print(f"è¡¨æ ¼: {stats['from_table']}")
print(f"JSON: {stats['from_json']}")
print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.2f}")
```

#### è¿‡æ»¤ä½ç½®ä¿¡åº¦
```python
extractions = UniversalParser.parse(html)

high_quality = [
    ext for ext in extractions
    if ext.confidence >= 0.7
]

print(f"é«˜è´¨é‡æ•°æ®: {len(high_quality)}/{len(extractions)}")
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **ç½®ä¿¡åº¦é˜ˆå€¼**ï¼šå»ºè®® >= 0.5ï¼Œä¸¥æ ¼åœºæ™¯ >= 0.7
2. **å»é‡**ï¼šè§£æå™¨å·²å†…ç½®å»é‡ï¼Œæ— éœ€é¢å¤–å¤„ç†
3. **é”™è¯¯å¤„ç†**ï¼šè§£æå¤±è´¥è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸æŠ›å¼‚å¸¸

---

## 3. StructureAnalyzer

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.structure_analyzer import StructureAnalyzer, Table, JSONBlock, HTMLList
```

### ğŸ“– ç±»å®šä¹‰

#### StructureAnalyzer

**ç±»æ–¹æ³•**ï¼š

##### analyze_all()
```python
@classmethod
def analyze_all(cls, html: str) -> Dict[str, Any]:
    """
    åˆ†æHTMLä¸­æ‰€æœ‰ç»“æ„
    
    è¿”å›:
        {
            "tables": List[Table],
            "json_blocks": List[JSONBlock],
            "lists": List[HTMLList],
            "text_blocks": List[str]
        }
    """
```

##### find_tables()
```python
@classmethod
def find_tables(cls, html: str) -> List[Table]:
    """æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼"""
```

##### find_json_blocks()
```python
@classmethod
def find_json_blocks(cls, html: str) -> List[JSONBlock]:
    """æŸ¥æ‰¾æ‰€æœ‰JSONå—"""
```

##### guess_column_index()
```python
@classmethod
def guess_column_index(cls, headers: List[str], field: str) -> Optional[int]:
    """
    çŒœæµ‹åˆ—ç´¢å¼•
    
    å‚æ•°:
        headers: è¡¨å¤´åˆ—è¡¨ ["IPåœ°å€", "ç«¯å£", "ç±»å‹"]
        field: å­—æ®µå "ip" / "port" / "protocol"
        
    è¿”å›:
        åˆ—ç´¢å¼•ï¼ˆ0-basedï¼‰æˆ– None
    """
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

#### åˆ†æç»“æ„
```python
from crawler.structure_analyzer import StructureAnalyzer

html = open("proxy_page.html").read()

# å®Œæ•´åˆ†æ
structure = StructureAnalyzer.analyze_all(html)

print(f"æ‰¾åˆ° {len(structure['tables'])} ä¸ªè¡¨æ ¼")
print(f"æ‰¾åˆ° {len(structure['json_blocks'])} ä¸ªJSONå—")
print(f"æ‰¾åˆ° {len(structure['lists'])} ä¸ªåˆ—è¡¨")

# éå†è¡¨æ ¼
for table in structure['tables']:
    print(f"\nè¡¨æ ¼ (ç½®ä¿¡åº¦: {table.confidence}):")
    print(f"  åˆ—: {', '.join(table.headers)}")
    print(f"  è¡Œæ•°: {len(table.rows)}")
```

#### æ™ºèƒ½åˆ—åŒ¹é…
```python
headers = ["IPåœ°å€", "ç«¯å£å·", "åè®®ç±»å‹", "å›½å®¶"]

ip_col = StructureAnalyzer.guess_column_index(headers, "ip")
port_col = StructureAnalyzer.guess_column_index(headers, "port")

print(f"IPåˆ—ç´¢å¼•: {ip_col}")      # 0
print(f"Portåˆ—ç´¢å¼•: {port_col}")  # 1
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **é¢„åˆ†æä¼˜åŒ–**ï¼šå…ˆåˆ†æç»“æ„ï¼Œå†ä¼ ç»™ UniversalParser
2. **ç½®ä¿¡åº¦è¿‡æ»¤**ï¼šå¿½ç•¥ä½äº 0.6 çš„ç»“æ„
3. **è¡¨æ ¼ä¼˜å…ˆ**ï¼šè¡¨æ ¼æ•°æ®è´¨é‡é€šå¸¸æœ€é«˜

---

## 4. PaginationDetector

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.pagination_detector import PaginationDetector, PaginationInfo, PaginationType
```

### ğŸ“– ç±»å®šä¹‰

#### PaginationDetector

**é™æ€æ–¹æ³•**ï¼š

##### detect_pagination()
```python
@staticmethod
def detect_pagination(html: str, base_url: str = '') -> PaginationInfo:
    """
    æ£€æµ‹åˆ†é¡µä¿¡æ¯
    
    å‚æ•°:
        html: HTMLå†…å®¹
        base_url: å½“å‰é¡µé¢URLï¼ˆç”¨äºæ¨æ–­å‚æ•°ï¼‰
        
    è¿”å›:
        PaginationInfo å¯¹è±¡
    """
```

##### detect_url_pattern()
```python
@staticmethod
def detect_url_pattern(url: str) -> Optional[URLPattern]:
    """
    ä»URLæ¨æ–­åˆ†é¡µæ¨¡å¼
    
    ç¤ºä¾‹:
        http://example.com/proxy?page=2
        æ¨æ–­: pageå‚æ•°, å½“å‰å€¼2, ä¸‹ä¸€é¡µ3
    """
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

#### åŸºç¡€æ£€æµ‹
```python
from crawler.pagination_detector import PaginationDetector

html = open("proxy_page.html").read()
current_url = "https://example.com/proxy?page=2"

info = PaginationDetector.detect_pagination(html, current_url)

if info.has_pagination:
    print(f"æ£€æµ‹åˆ°åˆ†é¡µ: {info.pagination_type.value}")
    print(f"ä¸‹ä¸€é¡µ: {info.next_page_url}")
    print(f"å½“å‰é¡µ: {info.current_page}")
    print(f"ç½®ä¿¡åº¦: {info.confidence:.2f}")
else:
    print("æœªæ£€æµ‹åˆ°åˆ†é¡µ")
```

####å®Œæ•´åˆ†é¡µçˆ¬å–
```python
def crawl_with_pagination(start_url: str, max_pages: int = 10):
    current_url = start_url
    page_num = 1
    
    while current_url and page_num <= max_pages:
        print(f"\nç¬¬ {page_num} é¡µ: {current_url}")
        
        # æŠ“å–é¡µé¢
        html = fetch_page(current_url)
        
        # æå–æ•°æ®
        proxies = UniversalParser.parse(html)
        print(f"  æå–: {len(proxies)} æ¡")
        
        # æ£€æµ‹åˆ†é¡µ
        info = PaginationDetector.detect_pagination(html, current_url)
        
        if info.has_pagination and info.next_page_url:
            current_url = info.next_page_url
            page_num += 1
        else:
            print("  æ— ä¸‹ä¸€é¡µï¼Œåœæ­¢")
            break
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **ç½®ä¿¡åº¦é˜ˆå€¼**ï¼š>= 0.7 æ‰ç»§ç»­ç¿»é¡µ
2. **å¾ªç¯æ£€æµ‹**ï¼šè®°å½•å·²è®¿é—®URLï¼Œé˜²æ­¢æ­»å¾ªç¯
3. **URLæ¨æ–­ä¼˜å…ˆ**ï¼šä¼ å…¥ `base_url` æé«˜å‡†ç¡®æ€§

---

## 5. PaginationController

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.pagination_controller import PaginationController, PaginationState
```

### ğŸ“– ç±»å®šä¹‰

#### PaginationController

**åˆå§‹åŒ–**ï¼š
```python
def __init__(self, max_pages: int = 10, max_pages_no_new_ip: int = 3):
    """
    å‚æ•°:
        max_pages: æœ€å¤§é¡µæ•°é™åˆ¶
        max_pages_no_new_ip: è¿ç»­æ— æ–°IPåœæ­¢é˜ˆå€¼
    """
```

**æ–¹æ³•**ï¼š

##### on_page_crawled()
```python
def on_page_crawled(self, new_ip_count: int) -> None:
    """
    è®°å½•é¡µé¢çˆ¬å–ç»“æœ
    
    å‚æ•°:
        new_ip_count: æœ¬é¡µæ–°å¢IPæ•°é‡
    """
```

##### should_continue()
```python
def should_continue(self) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”ç»§ç»­çˆ¬å–
    
    è¿”å›:
        True: ç»§ç»­, False: åœæ­¢
    """
```

##### get_state()
```python
def get_state(self) -> PaginationState:
    """è·å–å½“å‰çŠ¶æ€"""
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

```python
from crawler.pagination_controller import PaginationController

controller = PaginationController(
    max_pages=10,
    max_pages_no_new_ip=3
)

current_url = start_url
all_proxies = set()

while current_url and controller.should_continue():
    # çˆ¬å–é¡µé¢
    html = fetch_page(current_url)
    proxies = UniversalParser.parse(html)
    
    # å»é‡ç»Ÿè®¡
    before = len(all_proxies)
    all_proxies.update((p.ip, p.port) for p in proxies)
    new_count = len(all_proxies) - before
    
    # æ›´æ–°æ§åˆ¶å™¨
    controller.on_page_crawled(new_count)
    
    print(f"ç¬¬ {controller.get_state().current_page} é¡µ:")
    print(f"  æå–: {len(proxies)}")
    print(f"  æ–°å¢: {new_count}")
    print(f"  è¿ç»­æ— æ–°: {controller.get_state().pages_no_new_ip}")
    
    # æ£€æµ‹ä¸‹ä¸€é¡µ
    info = PaginationDetector.detect_pagination(html, current_url)
    current_url = info.next_page_url if info.has_pagination else None

print(f"\nåœæ­¢åŸå› : {controller.get_stop_reason()}")
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **åˆç†è®¾ç½®é˜ˆå€¼**ï¼š`max_pages_no_new_ip=3` è¾ƒä¸ºå¹³è¡¡
2. **ç›‘æ§çŠ¶æ€**ï¼šå®šæœŸæ£€æŸ¥ `get_state()` äº†è§£è¿›åº¦
3. **å¼‚å¸¸å¤„ç†**ï¼šç½‘ç»œé”™è¯¯æ—¶ä¹Ÿè°ƒç”¨ `on_page_crawled(0)`

---

## 6. LLMCaller

### ğŸ“¦ æ¨¡å—è·¯å¾„
```python
from crawler.llm_caller import LLMCaller
from crawler.llm_config import LLMConfig
```

### ğŸ“– ç±»å®šä¹‰

#### LLMCaller

**åˆå§‹åŒ–**ï¼š
```python
def __init__(self, config: LLMConfig):
    """
    å‚æ•°:
        config: LLMé…ç½®å¯¹è±¡
    """
```

**æ–¹æ³•**ï¼š

##### call_llm_for_parsing()
```python
def call_llm_for_parsing(
    self,
    html: str,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    è°ƒç”¨LLMè§£æHTML
    
    å‚æ•°:
        html: HTMLå†…å®¹
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        
    è¿”å›:
        {
            "proxies": [...],
            "cost_usd": 0.0003,
            "tokens": {"input": 1200, "output": 150},
            "cached": False
        }
    """
```

##### estimate_cost()
```python
def estimate_cost(self, input_tokens: int, output_tokens: int = 0) -> float:
    """
    ä¼°ç®—æˆæœ¬
    
    è¿”å›:
        æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    """
```

### ğŸ“ ä½¿ç”¨ç¤ºä¾‹

#### åŸºç¡€è°ƒç”¨
```python
from crawler.llm_config import LLMConfig
from crawler.llm_caller import LLMCaller

# åŠ è½½é…ç½®
config = LLMConfig.from_env()

# åˆ›å»ºè°ƒç”¨å™¨
caller = LLMCaller(config)

# è°ƒç”¨LLM
html = open("complex_page.html").read()
result = caller.call_llm_for_parsing(html)

if "error" not in result:
    print(f"æå–: {len(result['proxies'])} æ¡")
    print(f"æˆæœ¬: ${result['cost_usd']:.6f}")
    print(f"Tokens: {result['tokens']}")
else:
    print(f"é”™è¯¯: {result['error']}")
```

#### æˆæœ¬é¢„ä¼°
```python
# å½“ LLM_SUBMIT_FULL_HTML=false æ—¶ï¼ŒæŒ‰é…ç½®æˆªå–
snippet_chars = config.html_snippet_chars
html_snippet = html[:snippet_chars]
estimated_tokens = len(html_snippet) // 4
estimated_cost = caller.estimate_cost(estimated_tokens, 100)

print(f"é¢„ä¼°tokens: {estimated_tokens}")
print(f"é¢„ä¼°æˆæœ¬: ${estimated_cost:.6f}")

if estimated_cost < 0.01:  # 1ç¾åˆ†ä»¥ä¸‹
    result = caller.call_llm_for_parsing(html)
```

### ğŸ¯ æœ€ä½³å®è·µ

1. **æˆæœ¬é¢„ä¼°**ï¼šè°ƒç”¨å‰å…ˆä¼°ç®—
2. **é”™è¯¯å¤„ç†**ï¼šæ£€æŸ¥è¿”å›ä¸­çš„ `error` å­—æ®µ
3. **HTMLæäº¤ç­–ç•¥**ï¼šä½¿ç”¨ `.env` é…ç½® `LLM_SUBMIT_FULL_HTML/LLM_HTML_SNIPPET_CHARS`
4. **é‡è¯•æœºåˆ¶**ï¼šå·²å†…ç½®é‡è¯•ï¼Œæ— éœ€æ‰‹åŠ¨é‡è¯•

---

## 7-16. å…¶ä»–æ¨¡å—

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œå…¶ä»–æ¨¡å—çš„è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒï¼š

- **LLMCache**: è§ [LLM_INTEGRATION.md](./LLM_INTEGRATION.md)
- **ErrorHandler**: è§ [LLM_INTEGRATION.md](./LLM_INTEGRATION.md)
- **ProxyValidator**: è§ [FEATURES.md](./FEATURES.md#5-å¤šå±‚éªŒè¯ç³»ç»Ÿ)
- **HTTPValidator**: è§ [FEATURES.md](./FEATURES.md)
- **Pipeline**: è§ [ARCHITECTURE.md](./ARCHITECTURE.md)
- **Storage**: è§ [ARCHITECTURE.md](./ARCHITECTURE.md)

---

## ğŸ”§ é›†æˆç¤ºä¾‹

### å®Œæ•´åŠ¨æ€çˆ¬è™«æµç¨‹

```python
from crawler.runtime import load_settings
from crawler.dynamic_crawler import DynamicCrawler
from crawler.storage import get_mysql_connection

# 1. åŠ è½½é…ç½®
settings = load_settings(".env")

# 2. åˆ›å»ºçˆ¬è™«
crawler = DynamicCrawler(settings)

# 3. æ‰§è¡Œçˆ¬å–
result = crawler.crawl(
    url="https://example.com/proxy",
    max_pages=5,
    use_ai=False,
    verbose=True
)

# 4. è¾“å‡ºç»“æœ
print(f"""
çˆ¬å–å®Œæˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  é¡µæ•°: {result.pages_crawled}
  æå–: {result.extracted}
  æœ‰æ•ˆ: {result.valid}
  å­˜å‚¨: {result.stored}
""")

# 5. æŸ¥è¯¢æ•°æ®åº“
if result.session_id:
    conn = get_mysql_connection(settings)
    with conn.cursor() as cur:
        # æŸ¥è¯¢ä¼šè¯è¯¦æƒ…
        cur.execute(
            "SELECT * FROM crawl_sessions WHERE session_id = %s",
            (result.session_id,)
        )
        session = cur.fetchone()
        print(f"ä¼šè¯çŠ¶æ€: {session['status']}")
        
        # æŸ¥è¯¢é¡µé¢æ—¥å¿—
        cur.execute(
            "SELECT * FROM page_logs WHERE session_id = %s",
            (result.session_id,)
        )
        pages = cur.fetchall()
        print(f"é¡µé¢æ—¥å¿—: {len(pages)} æ¡")
```

### è‡ªå®šä¹‰è§£ææµç¨‹

```python
from crawler.structure_analyzer import StructureAnalyzer
from crawler.universal_parser import UniversalParser
from crawler.proxy_validator import ProxyValidator

# 1. åˆ†æç»“æ„
html = fetch_page(url)
structure = StructureAnalyzer.analyze_all(html)

# 2. è¿‡æ»¤é«˜è´¨é‡ç»“æ„
good_tables = [
    table for table in structure['tables']
    if table.confidence >= 0.8
]

# 3. æ‰‹åŠ¨æå–
extractions = []
for table in good_tables:
    extracted = UniversalParser.extract_from_tables([table])
    extractions.extend(extracted)

# 4. éªŒè¯
validator = ProxyValidator()
valid_proxies = []

for ext in extractions:
    proxy = {
        "ip": ext.ip,
        "port": ext.port,
        "protocol": ext.protocol or "http"
    }
    
    result = validator.validate_proxy(proxy)
    
    if result.is_valid:
        valid_proxies.append(proxy)
    else:
        print(f"æ— æ•ˆ: {proxy} - {result.anomalies}")

# 5. å­˜å‚¨
# ... (ä½¿ç”¨ Storage æ¨¡å—)
```

---

**ç›¸å…³æ–‡æ¡£**ï¼š
- ğŸ‘‰ [æ¶æ„è®¾è®¡](./ARCHITECTURE.md)
- ğŸ‘‰ [åŠŸèƒ½è¯¦è§£](./FEATURES.md)
- ğŸ‘‰ [API é›†æˆ](./UNIVERSAL_CRAWLER_API.md)
- ğŸ‘‰ [LLM é›†æˆ](./LLM_INTEGRATION.md)
