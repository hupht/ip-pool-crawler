# é€šç”¨åŠ¨æ€çˆ¬è™« - API æ–‡æ¡£

**ç‰ˆæœ¬**ï¼š1.0  
**æ—¥æœŸ**ï¼š2026-02-12

---

## ğŸ“š ç›®å½•

1. [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
2. [æ•°æ®ç»“æ„](#æ•°æ®ç»“æ„)
3. [ä¸»è¦ç±»å’Œæ–¹æ³•](#ä¸»è¦ç±»å’Œæ–¹æ³•)
4. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¨¡å—

> æ³¨ï¼šæœ¬èŠ‚ä»¥å½“å‰ä»“åº“ä¸­çš„å®é™…å®ç°ä¸ºå‡†ã€‚

### `crawler.llm_config`
LLM é…ç½®ç®¡ç†

### `crawler.universal_detector`
IP å’Œç«¯å£ç‰¹å¾æ£€æµ‹

### `crawler.structure_analyzer`
é¡µé¢ç»“æ„è¯†åˆ«

### `crawler.universal_parser`
é€šç”¨æ•°æ®è§£æ

### `crawler.pagination_detector`
åˆ†é¡µæ£€æµ‹

### `crawler.pagination_controller`
åˆ†é¡µæµç¨‹æ§åˆ¶

### `crawler.proxy_validator`
æ•°æ®éªŒè¯å’Œå¼‚å¸¸æ£€æµ‹

### `crawler.http_validator`
HTTP/TCP è¿é€šæ€§éªŒè¯

### `crawler.llm_caller`
LLM API è°ƒç”¨

### `crawler.llm_cache`
LLM ç»“æœç¼“å­˜

### `crawler.error_handler`
ä¸‰å±‚å®¹é”™åè°ƒ

### `crawler.dynamic_crawler`
ä¸»æ§åˆ¶å™¨

---

## æ•°æ®ç»“æ„

### `IPMatch`
```python
@dataclass
class IPMatch:
    ip: str                  # IP åœ°å€
    port: Optional[int]      # ç«¯å£ï¼ˆå¯é€‰ï¼‰
    protocol: Optional[str]  # åè®® (http/https/socks5 ç­‰)
    matched_text: str        # åŸå§‹åŒ¹é…æ–‡æœ¬
    position: Tuple[int, int]  # ä½ç½® (start, end)
    context: str             # å‘¨è¾¹ä¸Šä¸‹æ–‡
    confidence: float        # ç½®ä¿¡åº¦ 0-1
    source: str              # æ•°æ®æ¥æº (regex/table/json/html_list)
```

### `ProxyRecord`
```python
@dataclass
class ProxyRecord:
    ip: str
    port: int
    protocol: str = 'http'
    anonymity: Optional[str] = None
    country: Optional[str] = None
    extraction_method: str = 'heuristic'
    confidence: float = 0.5
    page_number: int = 1
    source_url: str = ''
```

### `DynamicCrawlResult`
```python
@dataclass
class DynamicCrawlResult:
    url: str
    pages_crawled: int
    extracted: int
    valid: int
    invalid: int
    stored: int
```

---

## ä¸»è¦ç±»å’Œæ–¹æ³•

### UniversalDetector

```python
class UniversalDetector:
    """IP å’Œç«¯å£ç‰¹å¾æ£€æµ‹"""
    
    @staticmethod
    def detect_ips(html: str) -> List[str]:
        """
        æ£€æµ‹ HTML ä¸­çš„æ‰€æœ‰ IP åœ°å€
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            IP åœ°å€åˆ—è¡¨
        """
        
    @staticmethod
    def detect_ip_port_pairs(html: str) -> List[Tuple[str, int]]:
        """
        æ£€æµ‹ HTML ä¸­çš„ IP:PORT å¯¹
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            (IP, PORT) å…ƒç»„åˆ—è¡¨
        """
        
    @staticmethod
    def detect_protocols(html: str) -> List[str]:
        """
        æ£€æµ‹ HTML ä¸­çš„åè®®å­—æ®µ
        
        Args:
            html: HTML å†…å®¹
            
        Returns:
            åè®®åˆ—è¡¨ (http, https, socks5 ç­‰)
        """
```

### StructureAnalyzer

```python
class StructureAnalyzer:
    """é¡µé¢ç»“æ„è¯†åˆ«"""
    
    @staticmethod
    def find_tables(html: str) -> List[dict]:
        """
        æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        
        Returns:
            è¡¨æ ¼åˆ—è¡¨ï¼Œæ¯ä¸ªè¡¨æ ¼åŒ…å«:
            {
                'headers': ['IP', 'ç«¯å£', 'åè®®'],
                'rows': [['1.2.3.4', 8080, 'http'], ...],
                'position': (start, end)
            }
        """
        
    @staticmethod
    def find_lists(html: str) -> List[dict]:
        """æŸ¥æ‰¾æ‰€æœ‰åˆ—è¡¨ç»“æ„"""
        
    @staticmethod
    def find_json_blocks(html: str) -> List[dict]:
        """æŸ¥æ‰¾ JSON æ•°æ®å—"""
```

### UniversalParser

```python
class UniversalParser:
    """é€šç”¨æ•°æ®è§£æ"""
    
    def parse(
        self, 
        html: str,
        structure: Optional[dict] = None,
        user_hint: Optional[str] = None
    ) -> List[ProxyRecord]:
        """
        è§£æ HTML å¹¶æå–ä»£ç†ä¿¡æ¯
        
        Args:
            html: HTML å†…å®¹
            structure: é¡µé¢ç»“æ„ï¼ˆç”± StructureAnalyzer æä¾›ï¼‰
            user_hint: ç”¨æˆ·æç¤ºï¼ˆå¦‚ "IP åœ¨ç¬¬ä¸€åˆ—"ï¼‰
            
        Returns:
            ä»£ç†è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« confidence
        """
        
    @staticmethod
    def calculate_confidence(
        extraction_source: str,
        field_presence: dict,
        context_certainty: float,
        format_validity: bool
    ) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦ (0-1)"""
```

### PaginationDetector

```python
class PaginationDetector:
    """åˆ†é¡µæ£€æµ‹"""
    
    @staticmethod
    def detect_url_pattern(url: str) -> Optional[dict]:
        """
        æ£€æµ‹ URL ä¸­çš„åˆ†é¡µå‚æ•°æ¨¡å¼
        
        Returns:
            {
                'pattern': 'page',      # å‚æ•°å
                'current_value': 1,
                'next_url': '...?page=2'
            }
        """
        
    @staticmethod
    def find_next_link(html: str) -> Optional[str]:
        """æ£€æµ‹ä¸‹ä¸€é¡µé“¾æ¥"""
        
    @staticmethod
    def find_load_more_button(html: str) -> Optional[dict]:
        """æ£€æµ‹åŠ è½½æ›´å¤šæŒ‰é’®"""
```

### PaginationController

```python
class PaginationController:
    """åˆ†é¡µæµç¨‹æ§åˆ¶"""
    
    def __init__(self, max_pages: int = 5, max_pages_no_new_ip: int = 3):
        """åˆå§‹åŒ–"""
        
    def should_continue(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­çˆ¬å–"""

    def mark_visited(self, url: str) -> bool:
        """æ ‡è®° URL å·²è®¿é—®ï¼›é‡å¤ URL è¿”å› False"""
        
    def get_next_url(self, current_url: str, detected_next_url: Optional[str]) -> Optional[str]:
        """è·å–ä¸‹ä¸€é¡µ URL"""
        
    def record_page_ips(self, ip_count: int) -> None:
        """è®°å½•å½“å‰é¡µ IP æ•°"""
        
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
```

### ProxyValidator

```python
class ProxyValidator:
    """æ•°æ®éªŒè¯å’Œå¼‚å¸¸æ£€æµ‹"""
    
    @staticmethod
    def validate_ip(ip: str) -> ValidationResult:
        """éªŒè¯ IP æ ¼å¼å’ŒèŒƒå›´"""
        
    @staticmethod
    def validate_port(port: Optional[int]) -> ValidationResult:
        """éªŒè¯ç«¯å£èŒƒå›´ (1-65535)"""
        
    @staticmethod
    def validate_proxy(
        ip: str,
        port: Optional[int] = None,
        protocol: Optional[str] = None
    ) -> ValidationResult:
        """éªŒè¯å®Œæ•´ä»£ç†è®°å½•"""
```

### LLMCaller

```python
class LLMCaller:
    """LLM API è°ƒç”¨"""
    
    def __init__(self, config: LLMConfig):
        """åˆå§‹åŒ–"""
        
    def call_llm_for_parsing(
        self,
        html: str,
        context: Optional[dict] = None
    ) -> dict:
        """
        è°ƒç”¨ LLM è§£æ HTML
        
        Returns:
            {
                'proxies': [
                    {
                        'ip': '1.2.3.4',
                        'port': 8080,
                        'protocol': 'http',
                        'confidence': 0.95,
                        'reasoning': '...'
                    }
                ]
            }
        """
        
    def estimate_cost(self, input_tokens: int, output_tokens: int = 0) -> float:
        """æŒ‰ token ä¼°ç®— API è°ƒç”¨æˆæœ¬ï¼ˆç¾å…ƒï¼‰"""
```

### ErrorHandler

```python
class ErrorHandler:
    """ä¸‰å±‚å®¹é”™åè°ƒ"""
    
    def process_page(
        self,
        html: str,
        context: Optional[dict] = None
    ) -> Tuple[List[dict], List[dict]]:
        """
        å¤„ç†å•é¡µçˆ¬å–
        
        Args:
            html: HTML å†…å®¹
            context: é¡µé¢ä¸Šä¸‹æ–‡
            
        Returns:
            (éªŒè¯é€šè¿‡çš„æ•°æ®, å¾…å®¡æŸ¥çš„æ•°æ®)
        """
        
    def should_use_ai(self, reason: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦è°ƒç”¨ AI"""
```

### DynamicCrawler

```python
class DynamicCrawler:
    """ä¸»æ§åˆ¶å™¨"""
    
    def crawl(
        self,
        url: str,
        max_pages: int = 1,
        use_ai: bool = False,
        no_store: bool = False,
        verbose: bool = False,
    ) -> DynamicCrawlResult:
        """
        çˆ¬å–æŒ‡å®š URL
        
        Args:
            url: èµ·å§‹ URL
            max_pages: æœ€å¤§é¡µæ•°
            use_ai: æ˜¯å¦å¯ç”¨ AIï¼ˆå½“å‰ç‰ˆæœ¬é¢„ç•™ï¼‰
            no_store: æ˜¯å¦è·³è¿‡ MySQL å†™å…¥
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
            
        Returns:
            çˆ¬å–ç»“æœ
        """
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ä¾‹å­ 1ï¼šæœ€ç®€å•çš„ç”¨æ³•

```python
from crawler.dynamic_crawler import DynamicCrawler
from crawler.runtime import load_settings

settings = load_settings()
crawler = DynamicCrawler(settings)
result = crawler.crawl('https://example.com/proxy')

print(f"æŠ“å–é¡µæ•°ï¼š{result.pages_crawled}")
print(f"æå–æ•°é‡ï¼š{result.extracted}")
```

### ä¾‹å­ 2ï¼šè‡ªå®šä¹‰é…ç½®

```python
from crawler.dynamic_crawler import DynamicCrawler
from crawler.runtime import load_settings

settings = load_settings()
crawler = DynamicCrawler(settings)
result = crawler.crawl(
    'https://example.com/proxy',
    max_pages=10,
    use_ai=True,
    no_store=True,
    verbose=True,
)
```

### ä¾‹å­ 3ï¼šæ£€æŸ¥çˆ¬å–ç»“æœ

```python
from crawler.dynamic_crawler import DynamicCrawler
from crawler.runtime import load_settings

settings = load_settings()
crawler = DynamicCrawler(settings)
result = crawler.crawl('https://example.com/proxy')

# è®¿é—®çˆ¬å–ç»Ÿè®¡
print(f"é¡µæ•°ï¼š{result.pages_crawled}")
print(f"æå–ï¼š{result.extracted}")
print(f"æœ‰æ•ˆï¼š{result.valid}")
print(f"æ— æ•ˆï¼š{result.invalid}")
print(f"å…¥åº“ï¼š{result.stored}")
```

### ä¾‹å­ 4ï¼šæ‰‹åŠ¨åˆ†é¡µæ§åˆ¶

```python
from crawler.pagination_detector import PaginationDetector
from crawler.pagination_controller import PaginationController
from crawler.fetcher import fetch

url = 'https://example.com/proxy?page=1'
controller = PaginationController(max_pages=5)

while controller.should_continue():
    if not controller.mark_visited(url):
        break

    # è·å–å½“å‰é¡µ
    html = fetch(url)
    
    # å¤„ç†é¡µé¢
    ips = extract_ips(html)
    controller.record_page_ips(len(ips))
    
    # æ‰¾ä¸‹ä¸€é¡µ
    detected_next_url = PaginationDetector.find_next_link(html)
    next_url = controller.get_next_url(url, detected_next_url)
    if next_url:
        url = next_url
    else:
        break
```

---

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ LLM æç¤ºè¯

```python
from crawler.llm_caller import LLMCaller

class CustomLLMCaller(LLMCaller):
    def _build_prompt(self, html: str, context: dict) -> str:
        return f"""
        åˆ†æä»¥ä¸‹ HTML é¡µé¢ï¼Œé‡ç‚¹æŸ¥æ‰¾ä»£ç† IP å’Œç«¯å£ã€‚
        ä¼˜å…ˆæŸ¥æ‰¾è¡¨æ ¼æ ¼å¼çš„æ•°æ®ã€‚
        
        HTML: {html[:3000]}
        
        è¿”å› JSON æ ¼å¼ã€‚
        """
```

### è‡ªå®šä¹‰æ•°æ®éªŒè¯

```python
from crawler.proxy_validator import ProxyValidator

class CustomValidator(ProxyValidator):
    @staticmethod
    def validate_proxy(ip: str, port: int) -> bool:
        # è‡ªå®šä¹‰éªŒè¯é€»è¾‘
        if ProxyValidator.validate_ip(ip).is_valid and ProxyValidator.validate_port(port).is_valid:
            # é¢å¤–æ£€æŸ¥
            if ip.startswith('192.168.'):
                return False  # ä¸æ¥å—ç§ç½‘ IP
            return True
        return False
```

---

## é”™è¯¯å¤„ç†

æ‰€æœ‰ API å¯èƒ½æŠ›å‡ºä»¥ä¸‹å¼‚å¸¸ï¼š

```python
class CrawlException(Exception):
    """çˆ¬å–å¼‚å¸¸åŸºç±»"""
    pass

class NetworkException(CrawlException):
    """ç½‘ç»œé”™è¯¯"""
    pass

class ParseException(CrawlException):
    """è§£æé”™è¯¯"""
    pass

class ValidateException(CrawlException):
    """éªŒè¯é”™è¯¯"""
    pass

class LLMException(CrawlException):
    """LLM API é”™è¯¯"""
    pass
```

ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
from crawler.dynamic_crawler import DynamicCrawler
from crawler.exceptions import CrawlException

try:
    result = crawler.crawl('https://example.com/proxy')
except CrawlException as e:
    print(f"çˆ¬å–å¤±è´¥ï¼š{e}")
```

---

## æ€§èƒ½æç¤º

1. **å¯ç”¨ç¼“å­˜**ï¼š`AI_CACHE_ENABLED=true` é¿å…é‡å¤è¯·æ±‚
2. **è°ƒæ•´å¹¶å‘**ï¼š`SOURCE_WORKERS=4` ä½†ä¸è¦è¿‡é«˜
3. **è®¾ç½®è¶…æ—¶**ï¼š`PAGE_FETCH_TIMEOUT_SECONDS=10` é˜²æ­¢å¡ä½
4. **æˆæœ¬æ§åˆ¶**ï¼š`AI_COST_LIMIT_USD=50` é™åˆ¶æ”¯å‡º

---

## ğŸ“ ç›¸å…³èµ„æº

- [é…ç½®æŒ‡å—](./UNIVERSAL_CRAWLER_CONFIG.md)
- [ä½¿ç”¨æŒ‡å—](./UNIVERSAL_CRAWLER_USAGE.md)
- [LLM é›†æˆ](./LLM_INTEGRATION.md)
- [æºä»£ç ](../crawler/)
