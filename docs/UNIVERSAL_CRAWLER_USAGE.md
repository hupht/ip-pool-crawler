# é€šç”¨åŠ¨æ€çˆ¬è™« - ä½¿ç”¨æŒ‡å—

**ç‰ˆæœ¬**ï¼š1.0  
**æ—¥æœŸ**ï¼š2026-02-12

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœ€ç®€å•çš„ç”¨æ³•

```bash
python cli.py crawl-custom https://example.com/proxy-list
```

ç³»ç»Ÿä¼šï¼š
1. ä¸‹è½½é¡µé¢ HTML
2. å¯å‘å¼è§£æï¼ˆè¡¨æ ¼ / JSON / åˆ—è¡¨ / æ–‡æœ¬ï¼‰
3. è‹¥æ— ç»“æœï¼Œè‡ªåŠ¨å°è¯•é¡µé¢æ¥å£å‘ç°ï¼ˆHTML + scriptï¼‰
4. è‹¥ä»æ— ç»“æœä¸”å·²å¯ç”¨è¿è¡Œæ—¶ sniffï¼ŒæŠ“å– XHR/FETCH JSON å“åº”
5. è‡ªåŠ¨è¯†åˆ«åˆ†é¡µå¹¶ç»§ç»­æŠ“å–
6. æ ¡éªŒã€å»é‡å¹¶æŒ‰ç­–ç•¥å…¥åº“
7. è¾“å‡ºæœ¬æ¬¡æŠ“å–ç»Ÿè®¡

### 2. äº¤äº’å¼æ¨¡å¼

```bash
python cli.py crawl-custom
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
æ¬¢è¿ä½¿ç”¨é€šç”¨åŠ¨æ€çˆ¬è™«äº¤äº’æ¨¡å¼

è¯·è¾“å…¥ç½‘å€: https://example.com/proxy-list
æœ€å¤§é¡µæ•° [5]: 
å¯ç”¨ AI è¾…åŠ© [y/N]: 
å¯ç”¨ JS æ¸²æŸ“æŠ“å–(Playwright) [y/N]: 
è‡ªåŠ¨å­˜å‚¨åˆ° MySQL [Y/n]: y

å¼€å§‹çˆ¬å–...

[é¡µé¢ 1/3] æ£€æµ‹ IP...
  âœ“ å·²æå– 45 ä¸ª IP
  ğŸ“„ æ£€æµ‹åˆ°ä¸‹ä¸€é¡µ
  
[é¡µé¢ 2/3] æ£€æµ‹ IP...
  âœ“ å·²æå– 42 ä¸ª IP
  ğŸ“„ æ£€æµ‹åˆ°ä¸‹ä¸€é¡µ

[é¡µé¢ 3/3] æ£€æµ‹ IP...
  âœ“ å·²æå– 38 ä¸ª IP
  â¹ï¸ æ²¡æœ‰æ›´å¤šé¡µé¢

çˆ¬å–å®Œæˆï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»è®¡ï¼š3 é¡µï¼Œ125 ä¸ªå”¯ä¸€ IP
å¹³å‡ç½®ä¿¡åº¦ï¼š0.87
å¾…å®¡æŸ¥ï¼š5 æ¡
å­˜å‚¨ï¼šMySQL âœ“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å¯æŒ‰éœ€æ‰§è¡Œ `python cli.py check` åšæ‰¹é‡ TCP æ£€æŸ¥ã€‚
```

---

## ğŸ“‹ å‘½ä»¤å‚è€ƒ

### åŸºç¡€è¯­æ³•

```bash
python cli.py crawl-custom [URL] [OPTIONS]
```

### é€‰é¡¹

| é€‰é¡¹ | è¯´æ˜ | é»˜è®¤ | ç¤ºä¾‹ |
|------|------|------|------|
| `--max-pages` | æœ€å¤§é¡µæ•° | 5 | `--max-pages 10` |
| `--use-ai` | å¯ç”¨ AI è¾…åŠ© | false | `--use-ai` |
| `--render-js` | ä½¿ç”¨ Playwright æ¸²æŸ“åè§£æ | false | `--render-js` |
| `--no-store` | ä¸å­˜å‚¨åˆ° MySQL | false | `--no-store` |
| `--verbose` | è¯¦ç»†æ—¥å¿— | false | `--verbose` |
| `--output-json` | å¯¼å‡º JSON ç»“æœæ–‡ä»¶ | æ—  | `--output-json result.json` |
| `--output-csv` | å¯¼å‡º CSV ç»“æœæ–‡ä»¶ | æ—  | `--output-csv result.csv` |

### è‡ªåŠ¨å›é€€é“¾è·¯ï¼ˆé»˜è®¤ï¼‰

`crawl-custom` çš„è§£ææ˜¯åˆ†å±‚å›é€€çš„ï¼Œä¸éœ€è¦æ–°å¢ CLI å‚æ•°ï¼š

1. é¡µé¢å¯å‘å¼è§£æï¼ˆé»˜è®¤ï¼‰
2. é¡µé¢æ¥å£è‡ªåŠ¨å‘ç°ï¼ˆ`API_DISCOVERY_*`ï¼‰
3. è¿è¡Œæ—¶æ¥å£æŠ“å–ï¼ˆ`RUNTIME_API_SNIFF_*`ï¼Œéœ€å¯ç”¨ä¸”ä»…åœ¨é `--render-js` è·¯å¾„ï¼‰

å¯¹åº”å…³ç³»ï¼š
- `--render-js`ï¼šä¸»åŠ¨èµ°â€œæµè§ˆå™¨æ¸²æŸ“ HTML â†’ è§£æâ€è·¯å¾„
- `RUNTIME_API_SNIFF_ENABLED=true`ï¼šåœ¨é `--render-js` ä¸”é™æ€é“¾è·¯æ— ç»“æœæ—¶ï¼Œå°è¯•æŠ“å–è¿è¡Œæ—¶ JSON å“åº”

æ¨èèµ·æ­¥é…ç½®ï¼ˆ`.env`ï¼‰ï¼š
```bash
API_DISCOVERY_ENABLED=true
API_DISCOVERY_MAX_SCRIPTS=6
API_DISCOVERY_MAX_CANDIDATES=12
RUNTIME_API_SNIFF_ENABLED=false
```

### å®Œæ•´ç¤ºä¾‹

```bash
# çˆ¬å–æœ€å¤š 10 é¡µï¼Œå¯ç”¨ AIï¼Œè¯¦ç»†æ—¥å¿—
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 10 \
  --use-ai \
  --verbose

# ä»…æ£€æµ‹ä¸å­˜å‚¨ï¼Œç½®ä¿¡åº¦è¦æ±‚é«˜
python cli.py crawl-custom https://example.com/proxy \
  --no-store

# å¿«é€Ÿçˆ¬å–
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 3 \
  --verbose

# å‰ç«¯æ¸²æŸ“ç«™ç‚¹ï¼ˆéœ€ Playwrightï¼‰
python cli.py crawl-custom https://www.iproyal.net/freeagency \
  --render-js \
  --max-pages 2 \
  --no-store \
  --verbose
```

---

## ğŸ¯ å¸¸è§åœºæ™¯

### åœºæ™¯ 1ï¼šå¿«é€Ÿæµ‹è¯•

**ç›®æ ‡**ï¼šå¿«é€ŸéªŒè¯ä¸€ä¸ªæ–°çš„ä»£ç†ç½‘ç«™

```bash
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 1 \
  --no-store \
  --verbose
```

ç»“æœï¼šåªçˆ¬é¦–é¡µï¼Œæ˜¾ç¤ºæ£€æµ‹åˆ°çš„ IPï¼Œä¸å­˜å‚¨ã€‚

---

### åœºæ™¯ 2ï¼šå®Œæ•´çˆ¬å– + äººå·¥å®¡æŸ¥

**ç›®æ ‡**ï¼šçˆ¬å–æ‰€æœ‰é¡µé¢ï¼Œä½†ä¿ç•™ä½è´¨æ•°æ®ä¾›å®¡æŸ¥

```bash
# åœ¨ .env ä¸­é…ç½®
SAVE_LOW_CONFIDENCE_DATA=true
REQUIRE_MANUAL_REVIEW=true

# ç„¶åçˆ¬å–
python cli.py crawl-custom https://example.com/proxy --verbose
```

**åç»­**ï¼š
1. æ£€æŸ¥ `proxy_review_queue` è¡¨
2. äººå·¥å®¡æŸ¥æ•°æ®
3. æ‰§è¡Œ SQL å°†å®¡æŸ¥è¿‡çš„æ•°æ®æ’å…¥ `proxy` è¡¨

---

### åœºæ™¯ 3ï¼šä½¿ç”¨ AI æ”¹å–„æ–°ç½‘ç«™

**ç›®æ ‡**ï¼šä¸ºåç»­ AI æå–æµç¨‹é¢„ç•™è¿è¡Œå‚æ•°

**å‰ç½®æ¡ä»¶**ï¼šé…ç½® LLMï¼ˆè§ [LLM é›†æˆæŒ‡å—](./LLM_INTEGRATION.md)ï¼‰

```bash
python cli.py crawl-custom https://newsite.com/proxy \
  --use-ai \
  --verbose
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
crawl-custom url=https://newsite.com/proxy pages=3 extracted=120 valid=98 stored=98
```

> è¯´æ˜ï¼š`--use-ai` å‚æ•°å·²æ¥å…¥ä¸»æµç¨‹ï¼›å½“è§¦å‘æ¡ä»¶æ»¡è¶³æ—¶ä¼šè°ƒç”¨ LLM å¹¶ä¸å¯å‘å¼ç»“æœåˆå¹¶ã€‚

---

### åœºæ™¯ 4ï¼šå¯¼å‡ºæŠ“å–ç»“æœåˆ°æ–‡ä»¶

```bash
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 3 \
  --output-json crawl_result.json \
  --output-csv crawl_result.csv
```

ç³»ç»Ÿä¼šåœ¨æ§åˆ¶å°è¾“å‡ºç»“æœï¼Œå¹¶åŒæ—¶å¯¼å‡º JSON/CSV ä¾›åç»­åˆ†æã€‚

---

### åœºæ™¯ 5ï¼šæŠ“å–å‰ç«¯æ¸²æŸ“ç«™ç‚¹

```bash
# é¦–æ¬¡ä½¿ç”¨å…ˆå®‰è£…ï¼ˆä¸€æ¬¡å³å¯ï¼‰
pip install playwright
python -m playwright install chromium

# å¼€å¯ JS æ¸²æŸ“æŠ“å–
python cli.py crawl-custom https://www.iproyal.net/freeagency --render-js --no-store --verbose
```

è¯´æ˜ï¼š`--render-js` ä¼šå…ˆç”¨æµè§ˆå™¨æ¸²æŸ“é¡µé¢ï¼Œå†æŠŠæ¸²æŸ“åçš„ HTML äº¤ç»™ç°æœ‰è§£ææµç¨‹ã€‚

---

### åœºæ™¯ 6ï¼šä»…é‡‡é›†ä¸å…¥åº“ï¼ˆå¹²è·‘ï¼‰

**ç›®æ ‡**ï¼šéªŒè¯é¡µé¢å¯è§£æï¼Œä½†ä¸å†™ MySQL

```bash
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 2 \
  --no-store \
  --verbose
```

ç³»ç»Ÿä¼šï¼š
1. æ­£å¸¸æŠ“å–ä¸è§£æ
2. æ˜¾ç¤º extracted/valid ç»Ÿè®¡
3. ä¸æ‰§è¡Œå…¥åº“

---

### åœºæ™¯ 7ï¼šé¡µé¢æ— æ˜æ–‡ä»£ç†ï¼Œè‡ªåŠ¨å‘ç°æ¥å£

**ç›®æ ‡**ï¼šé¡µé¢ HTML æ²¡æœ‰ç›´æ¥ IP:Portï¼Œä½†è„šæœ¬é‡Œæœ‰ API ç«¯ç‚¹

```bash
# åœ¨ .env ä¸­å»ºè®®é…ç½®
API_DISCOVERY_ENABLED=true
API_DISCOVERY_MAX_SCRIPTS=8
API_DISCOVERY_MAX_CANDIDATES=20
API_DISCOVERY_WHITELIST=proxy,ip,/api/,freeagency
API_DISCOVERY_BLACKLIST=ads,analytics,tracker

# æ‰§è¡ŒæŠ“å–
python cli.py crawl-custom https://example.com/freeagency \
  --no-store \
  --verbose
```

å¯èƒ½çœ‹åˆ°æ—¥å¿—ï¼š
```
crawl-custom api-discovery candidates=12
crawl-custom api-hit url=https://example.com/api/proxy records=50
```

---

### åœºæ™¯ 8ï¼šç­¾å/åŠ¨æ€ token æ¥å£ï¼Œå¯ç”¨è¿è¡Œæ—¶ sniff å›é€€

**ç›®æ ‡**ï¼šé™æ€ API å‘ç°æ‹¿ä¸åˆ°æ•°æ®ï¼Œä½†æµè§ˆå™¨è¿è¡Œæ—¶ç½‘ç»œé‡Œæœ‰ JSON å“åº”

```bash
# é¦–æ¬¡ä½¿ç”¨å…ˆå®‰è£…ï¼ˆä¸€æ¬¡å³å¯ï¼‰
pip install playwright
python -m playwright install chromium

# åœ¨ .env ä¸­å¯ç”¨è¿è¡Œæ—¶æŠ“å–
RUNTIME_API_SNIFF_ENABLED=true
RUNTIME_API_SNIFF_MAX_PAYLOADS=30
RUNTIME_API_SNIFF_MAX_RESPONSE_BYTES=300000

# æ‰§è¡ŒæŠ“å–ï¼ˆæ³¨æ„ï¼šæ­¤åœºæ™¯å»ºè®®ä¸è¦å¸¦ --render-jsï¼‰
python cli.py crawl-custom https://example.com/freeagency \
  --no-store \
  --verbose
```

å¯èƒ½çœ‹åˆ°æ—¥å¿—ï¼š
```
crawl-custom runtime-sniff records=50
```

è¯´æ˜ï¼šè‹¥åŒæ—¶ä½¿ç”¨ `--render-js`ï¼Œå½“å‰å®ç°ä¸ä¼šè§¦å‘è¿è¡Œæ—¶ sniff å›é€€ã€‚

---

## ğŸ“Š ç†è§£è¾“å‡º

### çˆ¬å–è¿‡ç¨‹æ—¥å¿—

```
crawl-custom url=https://example.com/proxy pages=3 extracted=125 valid=120 stored=120
```

**å«ä¹‰**ï¼š
- **pages**ï¼šå®é™…æŠ“å–é¡µæ•°
- **extracted**ï¼šè§£æå‡ºçš„å€™é€‰ä»£ç†æ•°é‡
- **valid**ï¼šé€šè¿‡æ ¡éªŒçš„ä»£ç†æ•°é‡
- **stored**ï¼šæˆåŠŸå†™å…¥ MySQL çš„æ•°é‡ï¼ˆ`--no-store` æ—¶ä¸º 0ï¼‰

### æœ€ç»ˆç»Ÿè®¡

```
çˆ¬å–å®Œæˆï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
pages=3
extracted=125
valid=120
stored=120
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**å«ä¹‰**ï¼š
- **extracted**ï¼šæå–æ€»é‡ï¼ˆå»é‡å‰ï¼‰
- **valid**ï¼šæ ¼å¼æ ¡éªŒé€šè¿‡ä¸”å¯å…¥åº“æ•°é‡
- **stored**ï¼šå†™å…¥æ•°æ®åº“æ•°é‡
- å½“æœªä½¿ç”¨ `--no-store` ä¸”æœ‰æ–°å…¥åº“æ•°æ®æ—¶ï¼Œä¼šè‡ªåŠ¨è§¦å‘åç»­ TCP æ‰¹é‡æ£€æŸ¥

---

## ğŸ” æŸ¥çœ‹ç»“æœ

### æŸ¥çœ‹å·²æå–çš„ IP

```bash
# MySQL ä¸­æŸ¥çœ‹
python -c "
from crawler.storage import Storage
storage = Storage.from_env()

# æŸ¥çœ‹æœ€è¿‘çˆ¬å–çš„ IP
ips = storage.connection.execute('''
    SELECT ip, port, protocol, protocol, is_available, score
    FROM proxy
    WHERE created_at > DATE_SUB(NOW(), INTERVAL 1 DAY)
    ORDER BY created_at DESC
    LIMIT 20
''').fetchall()

for ip in ips:
    print(ip)
"
```

### æŸ¥çœ‹å¾…å®¡æŸ¥æ•°æ®

```bash
python -c "
from crawler.storage import Storage
storage = Storage.from_env()

# æŸ¥çœ‹å¾…å®¡æŸ¥é˜Ÿåˆ—
reviews = storage.connection.execute('''
    SELECT id, ip, port, confidence, extraction_method, error_reason
    FROM proxy_review_queue
    WHERE status = 'pending'
    LIMIT 20
''').fetchall()

for review in reviews:
    print(review)
"
```

### æŸ¥çœ‹çˆ¬å–æ—¥å¿—

```bash
python -c "
from crawler.storage import Storage
storage = Storage.from_env()

# æŸ¥çœ‹æœ€è¿‘çš„çˆ¬å–æ—¥å¿—
logs = storage.connection.execute('''
    SELECT crawl_session_id, source_url, page_number, ip_count, 
           confidence_avg, error_message
    FROM crawl_page_log
    ORDER BY created_at DESC
    LIMIT 20
''').fetchall()

for log in logs:
    print(log)
"
```

---

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šæ— æ³•æ£€æµ‹ IP

**ç—‡çŠ¶**ï¼šçˆ¬å–å®Œæˆä½† IP æ•°ä¸º 0

**å¯èƒ½åŸå› **ï¼š
1. ç½‘é¡µç»“æ„ä¸åŒï¼ˆè¡¨æ ¼æ ¼å¼ã€JSON ç­‰ï¼‰
2. IP åœ°å€åœ¨ JavaScript ç”Ÿæˆçš„å†…å®¹ä¸­
3. ç½‘é¡µéœ€è¦ç™»å½•æˆ–éªŒè¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
python cli.py crawl-custom https://example.com/proxy \
  --verbose

# 2. æŸ¥çœ‹åŸå§‹ HTML
python -c "
from crawler.fetcher import fetch
html = fetch('https://example.com/proxy')
print(html[:2000])  # æ‰“å°å‰ 2000 å­—ç¬¦
"

# 3. ä½¿ç”¨ AI è¾…åŠ©
python cli.py crawl-custom https://example.com/proxy \
  --use-ai

# 4. æ£€æŸ¥é¡µé¢æ˜¯å¦éœ€è¦ User-Agent
# åœ¨ .env ä¸­ä¿®æ”¹
USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36

# 5. å¯ç”¨é¡µé¢æ¥å£è‡ªåŠ¨å‘ç°
API_DISCOVERY_ENABLED=true
API_DISCOVERY_WHITELIST=proxy,ip,/api/,freeagency

# 6. å¯¹ç­¾åæ¥å£å¯ç”¨è¿è¡Œæ—¶ sniffï¼ˆéœ€ Playwrightï¼‰
RUNTIME_API_SNIFF_ENABLED=true
RUNTIME_API_SNIFF_MAX_PAYLOADS=30
```

### é—®é¢˜ 2ï¼šåˆ†é¡µæ£€æµ‹å¤±è´¥

**ç—‡çŠ¶**ï¼šåªçˆ¬äº†ç¬¬ 1 é¡µï¼Œè™½ç„¶æœ‰æ›´å¤šé¡µé¢

**å¯èƒ½åŸå› **ï¼š
1. ä¸‹ä¸€é¡µ URL æ ¼å¼æœªè¢«è¯†åˆ«
2. åˆ†é¡µç”¨ JavaScript å®ç°

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. æ‰‹åŠ¨æŒ‡å®šé¡µæ•°æˆ– URL æ¨¡å¼
# åœ¨ .env ä¸­æ·»åŠ 
MAX_PAGES=1  # ä»…çˆ¬é¦–é¡µï¼Œå†æ‰‹åŠ¨åˆ†æ

# 2. æ£€æŸ¥é¡µé¢ä¸­çš„åˆ†é¡µé“¾æ¥
python -c "
from bs4 import BeautifulSoup
from crawler.fetcher import fetch

html = fetch('https://example.com/proxy')
soup = BeautifulSoup(html, 'html.parser')

# æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
links = soup.find_all('a')
for link in links:
    if 'ä¸‹ä¸€é¡µ' in link.text or 'next' in link.text.lower():
        print(f'æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥: {link.get(\"href\")}')
"

# 3. ä½¿ç”¨ JavaScript æ¸²æŸ“ï¼ˆå¯é€‰ï¼Œéœ€é¢å¤–é…ç½®ï¼‰
# è§é«˜çº§ç”¨æ³•
```

### é—®é¢˜ 3ï¼šå­˜å‚¨å¤±è´¥

**ç—‡çŠ¶**ï¼šçˆ¬å–æˆåŠŸï¼Œä½†æ•°æ®æœªå…¥åº“

**å¯èƒ½åŸå› **ï¼š
1. MySQL è¿æ¥å¤±è´¥
2. è¡¨ä¸å­˜åœ¨
3. æ•°æ®æ ¼å¼é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. æ£€æŸ¥ MySQL è¿æ¥
python -c "
from crawler.storage import Storage
storage = Storage.from_env()
print('âœ“ MySQL è¿æ¥æˆåŠŸ')
"

# 2. æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
python -c "
from crawler.storage import Storage
storage = Storage.from_env()
tables = storage.connection.execute(
    'SHOW TABLES'
).fetchall()
print(tables)
"

# 3. æ£€æŸ¥æ•°æ®æ ¼å¼
python -c "
from crawler.universal_parser import UniversalParser
parser = UniversalParser()
record = parser.parse('<html>1.2.3.4:8080</html>')
print(record)  # æ£€æŸ¥è¿”å›çš„æ•°æ®æ ¼å¼
"
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å¿«é€Ÿçˆ¬å–

```bash
# å°é¡µæ•° + ä¸å…¥åº“ï¼Œç”¨äºå¿«é€ŸéªŒè¯
python cli.py crawl-custom https://example.com/proxy \
  --max-pages 5 \
  --no-store
```

### ç²¾å‡†çˆ¬å–

```bash
# å¯ç”¨ AI å‚æ•°å¹¶æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
python cli.py crawl-custom https://example.com/proxy \
  --use-ai \
  --verbose
```

### æˆæœ¬æ§åˆ¶

```bash
# ç¦ç”¨ AIï¼ˆé¿å…æˆæœ¬ï¼‰
# åœ¨ .env ä¸­
USE_AI_FALLBACK=false

# æˆ–å¯ç”¨ AI ç¼“å­˜
AI_CACHE_ENABLED=true
AI_CACHE_TTL_HOURS=48
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [é…ç½®æŒ‡å—](./UNIVERSAL_CRAWLER_CONFIG.md)
- [LLM é›†æˆæŒ‡å—](./LLM_INTEGRATION.md)
- [API æ–‡æ¡£](./UNIVERSAL_CRAWLER_API.md)
- [æ•…éšœæ’æŸ¥](./TROUBLESHOOTING.md)
- [CLI å‚è€ƒ](./CLI_REFERENCE.md)
