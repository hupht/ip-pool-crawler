#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Deployment verification script.

Runs comprehensive checks for all features and writes a markdown report to reports/verify_report.md.
Optimized for quick verification: stops at first success for time-consuming operations.
"""

from __future__ import annotations

import json
import os
import sys
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import pymysql
import redis

from crawler.config import Settings
from crawler.runtime import load_settings
from crawler.fetcher import fetch_source
from crawler.parsers import (
    parse_geonode,
    parse_proxy_list_download_http,
    parse_proxy_list_download_https,
    parse_proxy_list_download_socks4,
    parse_proxy_list_download_socks5,
)
from crawler.sources import get_sources


REPORT_PATH = os.path.join("reports", "verify_report.md")


@dataclass
class CheckResult:
    name: str
    ok: bool
    details: str
    doc_refs: List[str] = None  # æ¨èçš„æ–‡æ¡£åˆ—è¡¨
    
    def __post_init__(self):
        if self.doc_refs is None:
            self.doc_refs = []


@dataclass
class SourceCheck:
    name: str
    url: str
    ok: bool
    reason: str
    status_code: int
    sample: Optional[Dict[str, Any]]
    doc_refs: List[str] = None
    
    def __post_init__(self):
        if self.doc_refs is None:
            self.doc_refs = []


def now_iso() -> str:
    return datetime.now().isoformat(sep=" ", timespec="seconds")


def check_python() -> CheckResult:
    return CheckResult(
        name="python",
        ok=True,
        details=f"python_version={sys.version.split()[0]}",
    )


def check_dependencies() -> CheckResult:
    missing = []
    modules = [
        "requests",
        "bs4",
        "pymysql",
        "redis",
        "dotenv",  # python-dotenv
    ]
    for module in modules:
        try:
            __import__(module)
        except Exception:
            missing.append(module)
    
    # æ£€æŸ¥å¯é€‰ä¾èµ–
    optional = []
    try:
        __import__("playwright")
        optional.append("playwright")
    except Exception:
        pass
    
    ok = len(missing) == 0
    details_parts = []
    if missing:
        details_parts.append(f"missing={','.join(missing)}")
    else:
        details_parts.append("core=âœ“")
    if optional:
        details_parts.append(f"optional={','.join(optional)}")
    
    details = ", ".join(details_parts) if details_parts else "all_present"
    doc_refs = ["docs/DEPLOYMENT.md", "docs/QUICK_START.md"] if not ok else []
    return CheckResult(name="dependencies", ok=ok, details=details, doc_refs=doc_refs)


def check_config(settings: Settings) -> CheckResult:
    missing = []
    if not settings.mysql_host:
        missing.append("MYSQL_HOST")
    if not settings.mysql_user:
        missing.append("MYSQL_USER")
    if not settings.mysql_database:
        missing.append("MYSQL_DATABASE")
    ok = len(missing) == 0
    details = "missing=" + ",".join(missing) if missing else "loaded"
    doc_refs = ["docs/DEPLOYMENT.md", ".env.example"] if not ok else []
    return CheckResult(name="config", ok=ok, details=details, doc_refs=doc_refs)


def check_mysql(settings: Settings) -> CheckResult:
    try:
        conn = pymysql.connect(
            host=settings.mysql_host,
            port=settings.mysql_port,
            user=settings.mysql_user,
            password=settings.mysql_password,
            database=settings.mysql_database,
            charset="utf8mb4",
            autocommit=True,
        )
        with conn.cursor() as cursor:
            cursor.execute("SHOW TABLES")
            tables = [row[0] for row in cursor.fetchall()]
            
            # æ£€æŸ¥æ ¸å¿ƒè¡¨
            required_tables = [
                "proxy_sources",
                "proxy_ips",
                "audit_logs",
                "crawl_session",
                "crawl_page_log",
                "proxy_review_queue",
                "llm_call_log",
            ]
            missing_tables = [t for t in required_tables if t not in tables]
            
            # ç»Ÿè®¡è®°å½•æ•°
            counts = {}
            for table in ["audit_logs", "proxy_ips", "crawl_session", "llm_call_log"]:
                if table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    counts[table] = int(cursor.fetchone()[0])
        
        conn.close()
        
        if missing_tables:
            return CheckResult(
                name="mysql",
                ok=False,
                details=f"missing_tables={','.join(missing_tables)}",
            )
        
        count_str = ", ".join([f"{k}={v}" for k, v in counts.items()])
        details = f"tables={len(tables)}, {count_str}"
        return CheckResult(name="mysql", ok=True, details=details)
    except Exception as exc:
        doc_refs = ["docs/DEPLOYMENT.md", "docs/TROUBLESHOOTING.md"]
        return CheckResult(name="mysql", ok=False, details=str(exc), doc_refs=doc_refs)


def check_redis(settings: Settings) -> CheckResult:
    try:
        client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            db=settings.redis_db,
            password=settings.redis_password or None,
            decode_responses=True,
        )
        client.ping()
        pool_size = int(client.zcard("proxy:alive"))
        return CheckResult(name="redis", ok=True, details=f"proxy:alive={pool_size}")
    except Exception as exc:
        doc_refs = ["docs/DEPLOYMENT.md", "docs/TROUBLESHOOTING.md"]
        return CheckResult(name="redis", ok=False, details=str(exc), doc_refs=doc_refs)


def check_logging(settings: Settings) -> CheckResult:
    try:
        from crawler.logging.logger import get_logger

        logger = get_logger(settings)
        logger.log_db_operation(
            operation="VERIFY",
            table="audit_logs",
            affected_rows=0,
            params={"verify": "deploy"},
        )
        return CheckResult(
            name="logging",
            ok=True,
            details=f"db_enabled={settings.log_db_write_enabled}",
        )
    except Exception as exc:
        doc_refs = ["docs/AUDIT_LOGGING.md"]
        return CheckResult(name="logging", ok=False, details=str(exc), doc_refs=doc_refs)


def parse_by_key(parser_key: str):
    mapping = {
        "proxy_list_download_http": parse_proxy_list_download_http,
        "proxy_list_download_https": parse_proxy_list_download_https,
        "proxy_list_download_socks4": parse_proxy_list_download_socks4,
        "proxy_list_download_socks5": parse_proxy_list_download_socks5,
        "geonode": parse_geonode,
    }
    return mapping.get(parser_key)


def check_sources(settings: Settings) -> List[SourceCheck]:
    """å¿«é€Ÿæ£€æŸ¥ï¼šåªè¦æœ‰ä¸€ä¸ªæºæˆåŠŸå°±é€šè¿‡"""
    results: List[SourceCheck] = []
    success_found = False
    doc_refs = ["docs/TROUBLESHOOTING.md", "crawler/sources.py"]
    
    for source in get_sources():
        # å¦‚æœå·²æ‰¾åˆ°æˆåŠŸçš„æºï¼Œè·³è¿‡å‰©ä½™ï¼ˆèŠ‚çœæ—¶é—´ï¼‰
        if success_found:
            results.append(
                SourceCheck(
                    name=source.name,
                    url=source.url,
                    ok=True,
                    reason="skipped",
                    status_code=0,
                    sample=None,
                )
            )
            continue
        
        body, status = fetch_source(source, settings)
        if not body or status == 0:
            results.append(
                SourceCheck(
                    name=source.name,
                    url=source.url,
                    ok=False,
                    reason="fetch_failed",
                    status_code=status,
                    sample=None,
                    doc_refs=doc_refs,
                )
            )
            continue

        parser = parse_by_key(source.parser_key)
        if not parser:
            results.append(
                SourceCheck(
                    name=source.name,
                    url=source.url,
                    ok=False,
                    reason="parser_not_found",
                    status_code=status,
                    sample=None,
                    doc_refs=doc_refs,
                )
            )
            continue

        records = parser(body)
        if not records:
            results.append(
                SourceCheck(
                    name=source.name,
                    url=source.url,
                    ok=False,
                    reason="no_records",
                    status_code=status,
                    sample=None,
                    doc_refs=doc_refs,
                )
            )
            continue

        sample = records[0]
        results.append(
            SourceCheck(
                name=source.name,
                url=source.url,
                ok=True,
                reason="ok",
                status_code=status,
                sample=sample,
            )
        )
        success_found = True  # æ‰¾åˆ°æˆåŠŸçš„æºï¼Œæ ‡è®°
    
    return results


def check_core_modules() -> CheckResult:
    """éªŒè¯æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯æ­£å¸¸å¯¼å…¥"""
    try:
        # åŸºç¡€æ¨¡å—å¯¼å…¥æµ‹è¯•
        from crawler import checker, config, fetcher, parsers, pipeline, runtime, sources, storage
        
        # å¯é€‰ï¼šå°è¯•å¯¼å…¥é«˜çº§æ¨¡å—ï¼ˆä¸å½±å“æ ¸å¿ƒéªŒè¯ï¼‰
        advanced_modules = []
        try:
            from crawler import (
                dynamic_crawler,
                error_handler,
                http_validator,
                llm_cache,
                llm_caller,
                llm_config,
                pagination_controller,
                pagination_detector,
                proxy_picker,
                proxy_validator,
                structure_analyzer,
                universal_detector,
                universal_parser,
                validator,
            )
            advanced_modules.append("advanced")
        except Exception:
            pass
        
        try:
            from crawler.logging import get_logger
            advanced_modules.append("logging")
        except Exception:
            pass
        
        modules_str = "+".join(advanced_modules) if advanced_modules else "core_only"
        return CheckResult(
            name="core_modules",
            ok=True,
            details=f"modules={modules_str}",
        )
    except Exception as exc:
        doc_refs = ["docs/MODULES.md", "docs/TROUBLESHOOTING.md"]
        return CheckResult(
            name="core_modules",
            ok=False,
            details=f"import_error: {str(exc)[:100]}",
            doc_refs=doc_refs,
        )


def check_dynamic_crawler(settings: Settings) -> CheckResult:
    """éªŒè¯åŠ¨æ€çˆ¬è™«åŠŸèƒ½ï¼ˆå¿«é€ŸéªŒè¯ï¼šæŠ“åˆ°1ä¸ªIPå°±æˆåŠŸï¼‰"""
    if not settings.dynamic_crawler_enabled:
        return CheckResult(
            name="dynamic_crawler",
            ok=True,
            details="disabled_in_config",
        )
    
    try:
        from crawler.dynamic_crawler import DynamicCrawler
        
        # éªŒè¯èƒ½å¦å®ä¾‹åŒ–
        crawler = DynamicCrawler(settings)
        
        return CheckResult(
            name="dynamic_crawler",
            ok=True,
            details=f"enabled={settings.dynamic_crawler_enabled}, max_pages={settings.max_pages}",
        )
    except ImportError as exc:
        # Python ç‰ˆæœ¬ä¸å…¼å®¹ï¼ˆéœ€è¦ 3.11+ï¼‰
        if "UTC" in str(exc) or "datetime" in str(exc):
            return CheckResult(
                name="dynamic_crawler",
                ok=True,  # æ ‡è®°ä¸ºé€šè¿‡ï¼Œä½†æ³¨æ˜éœ€è¦æ›´é«˜ç‰ˆæœ¬
                details="requires_python_3.11+",
            )
        raise
    except Exception as exc:
        doc_refs = ["docs/UNIVERSAL_CRAWLER_USAGE.md", "docs/UNIVERSAL_CRAWLER_CONFIG.md"]
        return CheckResult(
            name="dynamic_crawler",
            ok=False,
            details=f"error: {str(exc)[:80]}",
            doc_refs=doc_refs,
        )


def check_llm_integration(settings: Settings) -> CheckResult:
    """éªŒè¯ LLM é›†æˆé…ç½®ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰"""
    if not settings.use_ai_fallback:
        return CheckResult(
            name="llm_integration",
            ok=True,
            details="disabled_in_config",
        )
    
    try:
        from crawler.llm_config import LLMConfig
        from crawler.llm_caller import LLMCaller
        from crawler.llm_cache import LLMCache
        
        llm_config = LLMConfig.from_env()
        
        # æ£€æŸ¥å…³é”®é…ç½®
        issues = []
        if not llm_config.api_key or llm_config.api_key == "sk-your-key-here":
            issues.append("invalid_api_key")
        if not llm_config.base_url:
            issues.append("missing_base_url")
        if not llm_config.model:
            issues.append("missing_model")
        
        if issues:
            return CheckResult(
                name="llm_integration",
                ok=False,
                details=f"config_issues: {','.join(issues)}",
            )
        
        # éªŒè¯æ¨¡å—å¯å®ä¾‹åŒ–
        caller = LLMCaller(llm_config)
        cache = LLMCache()  # ä½¿ç”¨é»˜è®¤å‚æ•°
        
        return CheckResult(
            name="llm_integration",
            ok=True,
            details=f"model={llm_config.model[:30]}",
        )
    except ImportError as exc:
        # Python ç‰ˆæœ¬ä¸å…¼å®¹ï¼ˆéœ€è¦ 3.11+ï¼‰
        if "UTC" in str(exc) or "datetime" in str(exc):
            return CheckResult(
                name="llm_integration",
                ok=True,  # æ ‡è®°ä¸ºé€šè¿‡ï¼Œä½†æ³¨æ˜éœ€è¦æ›´é«˜ç‰ˆæœ¬
                details="requires_python_3.11+",
            )
        raise
    except Exception as exc:
        doc_refs = ["docs/LLM_INTEGRATION.md", ".env.example"]
        return CheckResult(
            name="llm_integration",
            ok=False,
            details=f"error: {str(exc)[:80]}",
            doc_refs=doc_refs,
        )


def check_pagination_system(settings: Settings) -> CheckResult:
    """éªŒè¯åˆ†é¡µæ£€æµ‹å’Œæ§åˆ¶ç³»ç»Ÿ"""
    try:
        # åªéªŒè¯æ¨¡å—å¯å¯¼å…¥ï¼Œä¸å®ä¾‹åŒ–ï¼ˆéœ€è¦ç‰¹å®šå‚æ•°ï¼‰
        from crawler.pagination_detector import PaginationDetector
        from crawler.pagination_controller import PaginationController
        
        return CheckResult(
            name="pagination",
            ok=True,
            details=f"max_pages={settings.max_pages}, dedup={settings.cross_page_dedup}",
        )
    except Exception as exc:
        doc_refs = ["docs/UNIVERSAL_CRAWLER_CONFIG.md"]
        return CheckResult(
            name="pagination",
            ok=False,
            details=f"error: {str(exc)[:80]}",
            doc_refs=doc_refs,
        )


def check_proxy_validators(settings: Settings) -> CheckResult:
    """éªŒè¯ä»£ç†éªŒè¯å™¨ï¼ˆTCP & HTTPï¼‰"""
    try:
        # éªŒè¯æ¨¡å—å­˜åœ¨
        from crawler import validator
        from crawler.http_validator import HTTPValidator
        # ProxyValidator å¯èƒ½éœ€è¦ç‰¹å®š Python ç‰ˆæœ¬
        try:
            from crawler.proxy_validator import ProxyValidator
            pv_available = True
        except Exception:
            pv_available = False
        
        details = f"tcp=âœ“, http=âœ“"
        if pv_available:
            details += f", workers={settings.validate_workers}"
        
        return CheckResult(
            name="proxy_validators",
            ok=True,
            details=details,
        )
    except Exception as exc:
        doc_refs = ["docs/MODULES.md"]
        return CheckResult(
            name="proxy_validators",
            ok=False,
            details=f"error: {str(exc)[:80]}",
            doc_refs=doc_refs,
        )


def check_universal_parser(settings: Settings) -> CheckResult:
    """éªŒè¯é€šç”¨è§£æå™¨ç³»ç»Ÿ"""
    try:
        # åªéªŒè¯æ¨¡å—å¯å¯¼å…¥
        from crawler.universal_parser import UniversalParser
        from crawler.universal_detector import UniversalDetector
        from crawler.structure_analyzer import StructureAnalyzer
        
        return CheckResult(
            name="universal_parser",
            ok=True,
            details=f"threshold={settings.heuristic_confidence_threshold}, struct_aware={settings.enable_struct_aware_parsing}",
        )
    except Exception as exc:
        doc_refs = ["docs/UNIVERSAL_CRAWLER_USAGE.md"]
        return CheckResult(
            name="universal_parser",
            ok=False,
            details=f"error: {str(exc)[:80]}",
            doc_refs=doc_refs,
        )


def check_cli_tools() -> CheckResult:
    """éªŒè¯ CLI å·¥å…·æ¨¡å—"""
    try:
        # éªŒè¯ CLI æ¨¡å—å­˜åœ¨
        import sys
        import os
        cli_path = os.path.join(os.path.dirname(__file__), "cli")
        if os.path.exists(cli_path):
            from cli.result_formatter import format_crawl_result
            return CheckResult(
                name="cli_tools",
                ok=True,
                details="formatter=âœ“",
            )
        else:
            return CheckResult(
                name="cli_tools",
                ok=True,
                details="cli_path_not_found(optional)",
            )
    except Exception as exc:
        return CheckResult(
            name="cli_tools",
            ok=True,  # CLI å·¥å…·æ˜¯å¯é€‰çš„
            details=f"optional_module: {str(exc)[:60]}",
        )


def render_report(
    started_at: str,
    finished_at: str,
    checks: List[CheckResult],
    source_checks: List[SourceCheck],
) -> str:
    ok_checks = sum(1 for c in checks if c.ok)
    total_checks = len(checks)
    ok_sources = sum(1 for s in source_checks if s.ok)
    total_sources = len(source_checks)

    lines: List[str] = []
    lines.append("# éƒ¨ç½²éªŒè¯æŠ¥å‘Š / Deployment Verification Report")
    lines.append("")
    lines.append(f"- started_at: {started_at}")
    lines.append(f"- finished_at: {finished_at}")
    lines.append(f"- checks_passed: {ok_checks}/{total_checks}")
    lines.append(f"- sources_passed: {ok_sources}/{total_sources}")
    lines.append("")

    lines.append("## ç³»ç»Ÿæ£€æŸ¥ / System Checks")
    lines.append("")
    
    # åˆ†ç»„æ˜¾ç¤º
    lines.append("### åŸºç¡€ç¯å¢ƒ")
    for check in checks:
        if check.name in ["python", "dependencies", "config"]:
            status = "âœ… PASS" if check.ok else "âŒ FAIL"
            lines.append(f"- **{check.name}**: {status}")
            lines.append(f"  - {check.details}")
            if not check.ok and check.doc_refs:
                lines.append(f"  - ğŸ“– æ¨èæ–‡æ¡£: {', '.join(check.doc_refs)}")
    
    lines.append("")
    lines.append("### æ•°æ®å­˜å‚¨")
    for check in checks:
        if check.name in ["mysql", "redis"]:
            status = "âœ… PASS" if check.ok else "âŒ FAIL"
            lines.append(f"- **{check.name}**: {status}")
            lines.append(f"  - {check.details}")
            if not check.ok and check.doc_refs:
                lines.append(f"  - ğŸ“– æ¨èæ–‡æ¡£: {', '.join(check.doc_refs)}")
    
    lines.append("")
    lines.append("### æ ¸å¿ƒåŠŸèƒ½æ¨¡å—")
    for check in checks:
        if check.name in ["core_modules", "logging", "universal_parser", "pagination", "proxy_validators"]:
            status = "âœ… PASS" if check.ok else "âŒ FAIL"
            lines.append(f"- **{check.name}**: {status}")
            lines.append(f"  - {check.details}")
            if not check.ok and check.doc_refs:
                lines.append(f"  - ğŸ“– æ¨èæ–‡æ¡£: {', '.join(check.doc_refs)}")
    
    lines.append("")
    lines.append("### é«˜çº§åŠŸèƒ½")
    for check in checks:
        if check.name in ["dynamic_crawler", "llm_integration", "cli_tools"]:
            status = "âœ… PASS" if check.ok else "âŒ FAIL"
            lines.append(f"- **{check.name}**: {status}")
            lines.append(f"  - {check.details}")
            if not check.ok and check.doc_refs:
                lines.append(f"  - ğŸ“– æ¨èæ–‡æ¡£: {', '.join(check.doc_refs)}")

    lines.append("")
    lines.append("## æ•°æ®æºæŠ½æ£€ / Source Fetch Sample")
    lines.append("")
    lines.append("*Note: å¿«é€ŸéªŒè¯æ¨¡å¼ - æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨æºåè·³è¿‡å‰©ä½™*")
    lines.append("")
    
    for src in source_checks:
        if src.reason == "skipped":
            lines.append(f"- â­ï¸ **{src.name}**: SKIPPED (å·²éªŒè¯å…¶ä»–æº)")
        else:
            status = "âœ… PASS" if src.ok else "âŒ FAIL"
            lines.append(f"- {status} **{src.name}**")
            lines.append(f"  - URL: {src.url}")
            lines.append(f"  - Status: {src.reason} (HTTP {src.status_code})")
            if src.sample:
                sample_str = json.dumps(src.sample, ensure_ascii=False)
                lines.append(f"  - Sample: `{sample_str}`")
            if not src.ok and src.doc_refs:
                lines.append(f"  - ğŸ“– æ¨èæ–‡æ¡£: {', '.join(src.doc_refs)}")

    lines.append("")
    lines.append("## æ€»ç»“ / Summary")
    lines.append("")
    
    all_pass = ok_checks == total_checks and ok_sources > 0  # è‡³å°‘ä¸€ä¸ªæºæˆåŠŸ
    if all_pass:
        lines.append("### âœ… éƒ¨ç½²éªŒè¯é€šè¿‡ / DEPLOYMENT VERIFIED")
        lines.append("")
        lines.append("æ‰€æœ‰ç³»ç»Ÿæ£€æŸ¥é€šè¿‡ï¼Œæ•°æ®æºå¯æ­£å¸¸æŠ“å–ã€‚ç³»ç»Ÿå·²å°±ç»ªï¼")
        lines.append("")
        lines.append("#### ä¸‹ä¸€æ­¥")
        lines.append("- è¿è¡Œçˆ¬è™«: `python main.py` æˆ– `python cli.py run`")
        lines.append("- æŸ¥çœ‹æ–‡æ¡£: `docs/INDEX.md` (å®Œæ•´æ–‡æ¡£å¯¼èˆª)")
        lines.append("- å¿«é€Ÿå¼€å§‹: `docs/QUICK_START.md`")
    else:
        lines.append("### âŒ éƒ¨ç½²éªŒè¯å¤±è´¥ / DEPLOYMENT FAILED")
        lines.append("")
        failed_checks = [c.name for c in checks if not c.ok]
        if failed_checks:
            lines.append(f"**å¤±è´¥çš„æ£€æŸ¥é¡¹**: {', '.join(failed_checks)}")
            lines.append("")
        if ok_sources == 0:
            lines.append("**è­¦å‘Š**: æ‰€æœ‰æ•°æ®æºæŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæº URL å¯ç”¨æ€§")
            lines.append("")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£æ¨è
        all_doc_refs = set()
        for check in checks:
            if not check.ok and check.doc_refs:
                all_doc_refs.update(check.doc_refs)
        for src in source_checks:
            if not src.ok and src.doc_refs:
                all_doc_refs.update(src.doc_refs)
        
        if all_doc_refs:
            lines.append("#### ğŸ“– æ•…éšœæ’æŸ¥æ–‡æ¡£æ¨è")
            lines.append("")
            for doc in sorted(all_doc_refs):
                lines.append(f"- `{doc}`")
            lines.append("")
        
        lines.append("#### ğŸ”§ å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ")
        lines.append("")
        
        # æä¾›é’ˆå¯¹æ€§çš„è§£å†³å»ºè®®
        if any(c.name == "dependencies" and not c.ok for c in checks):
            lines.append("**ä¾èµ–é—®é¢˜**:")
            lines.append("```bash")
            lines.append("pip install -r requirements.txt")
            lines.append("```")
            lines.append("")
        
        if any(c.name == "config" and not c.ok for c in checks):
            lines.append("**é…ç½®é—®é¢˜**:")
            lines.append("```bash")
            lines.append("cp .env.example .env")
            lines.append("# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å†™æ•°æ®åº“è¿æ¥ä¿¡æ¯")
            lines.append("```")
            lines.append("")
        
        if any(c.name == "mysql" and not c.ok for c in checks):
            lines.append("**MySQL è¿æ¥é—®é¢˜**:")
            lines.append("- ç¡®è®¤ MySQL æœåŠ¡å·²å¯åŠ¨")
            lines.append("- æ£€æŸ¥ .env ä¸­çš„è¿æ¥å‚æ•° (HOST, PORT, USER, PASSWORD)")
            lines.append("- æ•°æ®åº“å’Œè¡¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼Œæ— éœ€æ‰‹åŠ¨æ‰§è¡Œ SQL")
            lines.append("")
        
        if any(c.name == "redis" and not c.ok for c in checks):
            lines.append("**Redis è¿æ¥é—®é¢˜**:")
            lines.append("- ç¡®è®¤ Redis æœåŠ¡å·²å¯åŠ¨")
            lines.append("- æ£€æŸ¥ .env ä¸­çš„ REDIS_HOST å’Œ REDIS_PORT")
            lines.append("- Windows: ä¸‹è½½ Redis for Windows æˆ–ä½¿ç”¨ WSL")
            lines.append("")
        
        if ok_sources == 0:
            lines.append("**æ•°æ®æºæ— æ³•è®¿é—®**:")
            lines.append("- æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
            lines.append("- éƒ¨åˆ†æºå¯èƒ½æš‚æ—¶ä¸å¯ç”¨ï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
            lines.append("- è‡³å°‘ä¿è¯ä¸€ä¸ªæºå¯ç”¨å³å¯æ­£å¸¸è¿è¡Œ")
            lines.append("")
        
        lines.append("#### ğŸ“š å®Œæ•´æ–‡æ¡£ç´¢å¼•")
        lines.append("")
        lines.append("- [æ–‡æ¡£æ€»å¯¼èˆª](docs/INDEX.md)")
        lines.append("- [å¿«é€Ÿå¼€å§‹](docs/QUICK_START.md)")
        lines.append("- [éƒ¨ç½²æŒ‡å—](docs/DEPLOYMENT.md)")
        lines.append("- [æ•…éšœæ’æŸ¥](docs/TROUBLESHOOTING.md)")

    return "\n".join(lines) + "\n"


def main(env_path: Optional[str] = None) -> int:
    started_at = now_iso()
    settings = load_settings(env_path) if env_path else Settings.from_env()

    print("ğŸ” å¼€å§‹éƒ¨ç½²éªŒè¯ / Starting deployment verification...")
    print()
    
    # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    print("ğŸ“¦ æ£€æŸ¥åŸºç¡€ç¯å¢ƒ...")
    checks = [
        check_python(),
        check_dependencies(),
        check_config(settings),
    ]
    
    # æ•°æ®å­˜å‚¨æ£€æŸ¥
    print("ğŸ’¾ æ£€æŸ¥æ•°æ®å­˜å‚¨...")
    checks.extend([
        check_mysql(settings),
        check_redis(settings),
    ])
    
    # æ ¸å¿ƒæ¨¡å—æ£€æŸ¥
    print("ğŸ”§ æ£€æŸ¥æ ¸å¿ƒåŠŸèƒ½æ¨¡å—...")
    checks.extend([
        check_core_modules(),
        check_logging(settings),
        check_universal_parser(settings),
        check_pagination_system(settings),
        check_proxy_validators(settings),
    ])
    
    # é«˜çº§åŠŸèƒ½æ£€æŸ¥
    print("ğŸš€ æ£€æŸ¥é«˜çº§åŠŸèƒ½...")
    checks.extend([
        check_dynamic_crawler(settings),
        check_llm_integration(settings),
        check_cli_tools(),
    ])

    # æ•°æ®æºæ£€æŸ¥ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸå°±åœæ­¢ï¼‰
    print("ğŸŒ æ£€æŸ¥æ•°æ®æºï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰...")
    source_checks = check_sources(settings)
    
    finished_at = now_iso()

    # ç”ŸæˆæŠ¥å‘Š
    report = render_report(started_at, finished_at, checks, source_checks)

    os.makedirs(os.path.dirname(REPORT_PATH), exist_ok=True)
    with open(REPORT_PATH, "w", encoding="utf-8") as fh:
        fh.write(report)

    # è®¡ç®—ç»“æœ
    ok_checks = all(c.ok for c in checks)
    ok_sources = any(s.ok for s in source_checks)  # è‡³å°‘ä¸€ä¸ªæºæˆåŠŸ

    print()
    print("=" * 60)
    print("éªŒè¯å®Œæˆ / Verification completed")
    print("=" * 60)
    print(f"ğŸ“„ æŠ¥å‘Šä½ç½®: {REPORT_PATH}")
    print()
    print(f"âœ… ç³»ç»Ÿæ£€æŸ¥: {sum(1 for c in checks if c.ok)}/{len(checks)} é€šè¿‡")
    print(f"âœ… æ•°æ®æº: {sum(1 for s in source_checks if s.ok)}/{len(source_checks)} å¯ç”¨")
    print()
    
    if ok_checks and ok_sources:
        print("ğŸ‰ éƒ¨ç½²éªŒè¯é€šè¿‡ï¼ç³»ç»Ÿå·²å°±ç»ªã€‚")
        return 0
    else:
        print("âš ï¸  éƒ¨ç½²éªŒè¯å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šè¯¦æƒ…ã€‚")
        if not ok_checks:
            failed = [c.name for c in checks if not c.ok]
            print(f"   å¤±è´¥é¡¹: {', '.join(failed)}")
        if not ok_sources:
            print("   è­¦å‘Š: æ‰€æœ‰æ•°æ®æºä¸å¯ç”¨")
        return 1


if __name__ == "__main__":
    sys.exit(main())
