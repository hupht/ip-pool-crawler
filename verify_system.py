#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""ç³»ç»ŸåŠŸèƒ½å®Œæ•´éªŒè¯è„šæœ¬"""

from crawler.config import Settings
from crawler.storage import get_mysql_connection
import redis
import sys

def test_mysql_connection():
    """æµ‹è¯• MySQL è¿æ¥"""
    print("\n=== æµ‹è¯• 1: MySQL è¿æ¥ ===")
    try:
        s = Settings.from_env()
        conn = get_mysql_connection(s)
        print(f"âœ… MySQL è¿æ¥æˆåŠŸ: {s.mysql_host}:{s.mysql_port}")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        with conn.cursor() as cursor:
            cursor.execute("SHOW TABLES")
            tables = [row[0] for row in cursor.fetchall()]
            print(f"âœ… å­˜åœ¨çš„è¡¨: {', '.join(tables)}")
            
            # æ£€æŸ¥ audit_logs è¡¨
            if 'audit_logs' in tables:
                cursor.execute("SELECT COUNT(*) FROM audit_logs")
                count = cursor.fetchone()[0]
                print(f"âœ… audit_logs è¡¨ä¸­çš„è®°å½•æ•°: {count}")
            else:
                print("âš ï¸  audit_logs è¡¨è¿˜ä¸å­˜åœ¨")
        
        conn.close()
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_redis_connection():
    """æµ‹è¯• Redis è¿æ¥"""
    print("\n=== æµ‹è¯• 2: Redis è¿æ¥ ===")
    try:
        s = Settings.from_env()
        r = redis.Redis(
            host=s.redis_host,
            port=s.redis_port,
            db=s.redis_db,
            password=s.redis_password if s.redis_password else None,
            decode_responses=True
        )
        r.ping()
        print(f"âœ… Redis è¿æ¥æˆåŠŸ: {s.redis_host}:{s.redis_port}")
        
        # æ£€æŸ¥ä»£ç†æ± å¤§å°
        pool_size = r.zcard("proxy:alive")
        print(f"âœ… ä»£ç†æ± å¤§å°: {pool_size} ä¸ªä»£ç†")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_logger():
    """æµ‹è¯•æ—¥å¿—ç³»ç»Ÿ"""
    print("\n=== æµ‹è¯• 3: æ—¥å¿—ç³»ç»Ÿ ===")
    try:
        from crawler.logging.logger import get_logger
        from crawler.config import Settings
        
        s = Settings.from_env()
        logger = get_logger()
        
        print(f"âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        print(f"   - æ•°æ®åº“æ—¥å¿—å¯ç”¨: {s.log_db_write_enabled}")
        print(f"   - æ—¥å¿—çº§åˆ«: {s.log_level}")
        print(f"   - æ—¥å¿—æ–‡ä»¶: {s.log_file_path}")
        
        # æµ‹è¯•è„±æ•åŠŸèƒ½
        logger.log_db_operation(
            operation="TEST",
            table="test_table",
            affected_rows=1,
            params={"password": "secret123", "ip": "192.168.1.100"}
        )
        print(f"âœ… æ—¥å¿—æµ‹è¯•è®°å½•å·²å†™å…¥")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_sources():
    """æµ‹è¯•æ•°æ®æº"""
    print("\n=== æµ‹è¯• 4: æ•°æ®æº ===")
    try:
        from crawler.sources import get_sources
        
        sources = get_sources()
        print(f"âœ… æ•°æ®æºåŠ è½½æˆåŠŸï¼Œå…± {len(sources)} ä¸ªæº:")
        for src in sources:
            print(f"   - {src.name} ({src.url})")
        
        return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_parsers():
    """æµ‹è¯•è§£æå™¨"""
    print("\n=== æµ‹è¯• 5: è§£æå™¨ ===")
    try:
        from pathlib import Path
        from crawler.parsers import parse_free_proxy_list
        
        # è¯»å–æµ‹è¯•æ•°æ®
        fixture_path = Path("tests/fixtures/free-proxy-list.html")
        if fixture_path.exists():
            html = fixture_path.read_text(encoding="utf-8")
            records = parse_free_proxy_list(html)
            if records:
                print(f"âœ… è§£æå™¨æ­£å¸¸ï¼Œè§£æåˆ° {len(records)} æ¡è®°å½•")
                sample = records[0]
                print(f"   ç¤ºä¾‹: {sample['ip']}:{sample['port']} ({sample.get('protocol', 'N/A')})")
                return True
            else:
                print("âš ï¸  è§£æåˆ° 0 æ¡è®°å½•")
                return True
        else:
            print(f"âš ï¸  æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {fixture_path}")
            return True
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_validator():
    """æµ‹è¯•éªŒè¯å™¨"""
    print("\n=== æµ‹è¯• 6: éªŒè¯å™¨ ===")
    try:
        from crawler.validator import score_proxy, tcp_check
        
        # æµ‹è¯•è¯„åˆ†
        score1 = score_proxy(latency_ms=100, success=True)
        score2 = score_proxy(latency_ms=500, success=True)
        score3 = score_proxy(latency_ms=100, success=False)
        
        if score1 > score2 and score1 > score3:
            print(f"âœ… è¯„åˆ†é€»è¾‘æ­£å¸¸")
            print(f"   - ä½å»¶è¿Ÿ+æˆåŠŸ: {score1:.2f}")
            print(f"   - é«˜å»¶è¿Ÿ+æˆåŠŸ: {score2:.2f}")
            print(f"   - ä½å»¶è¿Ÿ+å¤±è´¥: {score3:.2f}")
            return True
        else:
            print(f"âŒ è¯„åˆ†é€»è¾‘å¼‚å¸¸")
            return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_configuration():
    """æµ‹è¯•é…ç½®"""
    print("\n=== æµ‹è¯• 7: é…ç½®ç³»ç»Ÿ ===")
    try:
        from crawler.config import Settings
        
        s = Settings.from_env()
        
        # æ£€æŸ¥å…³é”®é…ç½®
        checks = [
            ("MySQL ä¸»æœº", s.mysql_host),
            ("MySQL ç”¨æˆ·", s.mysql_user),
            ("Redis ä¸»æœº", s.redis_host),
            ("æ—¥å¿—å¯ç”¨", s.log_db_write_enabled),
            ("å¹¶å‘è®¾ç½®", s.source_workers > 0),
        ]
        
        all_ok = True
        for name, value in checks:
            if value:
                print(f"âœ… {name}: {value}")
            else:
                print(f"âŒ {name}: æœªé…ç½®")
                all_ok = False
        
        return all_ok
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("IP Pool Crawler - ç³»ç»ŸåŠŸèƒ½å®Œæ•´éªŒè¯")
    print("="*60)
    
    results = []
    
    results.append(("MySQL è¿æ¥", test_mysql_connection()))
    results.append(("Redis è¿æ¥", test_redis_connection()))
    results.append(("æ—¥å¿—ç³»ç»Ÿ", test_logger()))
    results.append(("æ•°æ®æº", test_sources()))
    results.append(("è§£æå™¨", test_parsers()))
    results.append(("éªŒè¯å™¨", test_validator()))
    results.append(("é…ç½®ç³»ç»Ÿ", test_configuration()))
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("éªŒè¯æ€»ç»“")
    print("="*60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status}: {name}")
    
    print(f"\nç»“æœ: {passed}/{total} é¡¹é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½æ­£å¸¸ï¼")
        return 0
    else:
        print(f"âš ï¸  æœ‰ {total - passed} é¡¹éœ€è¦ä¿®å¤")
        return 1


if __name__ == "__main__":
    sys.exit(main())
