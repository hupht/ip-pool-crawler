"""
REST API æœåŠ¡å™¨ï¼Œç”¨äºæš´éœ² IP ä»£ç†æ± çˆ¬è™«çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
é€šè¿‡ `python cli.py server` å¯åŠ¨æœåŠ¡ã€‚
"""

from __future__ import annotations

import asyncio
from typing import Any, Optional
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, HttpUrl
import uvicorn

from crawler.dynamic_crawler import DynamicCrawler, crawl_custom_url, DynamicCrawlResult
from crawler.pipeline import run_once
from crawler.runtime import load_settings
from crawler.proxy_picker import pick_proxies
from tools import check_pool, diagnose_sources, diagnose_pipeline, get_proxy


# ============ æ•°æ®æ¨¡å‹ ============

class CrawlCustomRequest(BaseModel):
    """è‡ªå®šä¹‰çˆ¬å–è¯·æ±‚"""
    url: HttpUrl = Field(..., description="ç›®æ ‡URL")
    max_pages: Optional[int] = Field(1, description="æœ€å¤§é¡µæ•°", ge=1)
    use_ai: bool = Field(False, description="å¯ç”¨AIè¾…åŠ©")
    render_js: bool = Field(False, description="å¯ç”¨JSæ¸²æŸ“(Playwright)")
    no_store: bool = Field(False, description="ä¸å­˜å‚¨åˆ°MySQL")
    verbose: bool = Field(False, description="è¯¦ç»†è¾“å‡º")


class CrawlCustomResponse(BaseModel):
    """è‡ªå®šä¹‰çˆ¬å–å“åº”"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    url: str = Field(..., description="çˆ¬å–çš„URL")
    session_id: Optional[int] = Field(None, description="ä¼šè¯ID")
    total_ips: int = Field(0, description="æå–çš„IPæ€»æ•°")
    stored: int = Field(0, description="å­˜å‚¨çš„IPæ•°é‡")
    avg_confidence: float = Field(0.0, description="å¹³å‡ç½®ä¿¡åº¦")
    ai_calls_count: int = Field(0, description="AIè°ƒç”¨æ¬¡æ•°")
    llm_cost_usd: float = Field(0.0, description="LLMæˆæœ¬ï¼ˆç¾å…ƒï¼‰")
    review_pending_count: int = Field(0, description="å¾…å®¡æ ¸æ•°é‡")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")


class GetProxyRequest(BaseModel):
    """è·å–ä»£ç†è¯·æ±‚"""
    count: int = Field(1, description="ä»£ç†æ•°é‡", ge=1, le=1000)
    protocol: Optional[str] = Field(None, description="åè®®ç±»å‹: http, https, socks4, socks5")
    country: Optional[str] = Field(None, description="å›½å®¶ä»£ç  (å¦‚ US, CN)")
    min_score: Optional[int] = Field(None, description="æœ€å°åˆ†æ•°", ge=0, le=100)
    format: str = Field("json", description="è¾“å‡ºæ ¼å¼: json, txt, csv")


class GetProxyResponse(BaseModel):
    """è·å–ä»£ç†å“åº”"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    count: int = Field(..., description="è¿”å›çš„ä»£ç†æ•°é‡")
    proxies: list[dict[str, Any]] = Field(..., description="ä»£ç†åˆ—è¡¨")


class RunCrawlerRequest(BaseModel):
    """è¿è¡Œçˆ¬è™«è¯·æ±‚"""
    quick_test: bool = Field(False, description="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    quick_record_limit: int = Field(1, description="å¿«é€Ÿæ¨¡å¼è®°å½•é™åˆ¶", ge=1)


class RunCrawlerResponse(BaseModel):
    """è¿è¡Œçˆ¬è™«å“åº”"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="æ‰§è¡Œæ¶ˆæ¯")


class CheckResponse(BaseModel):
    """ä»£ç†æ£€æŸ¥å“åº”"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="æ‰§è¡Œæ¶ˆæ¯")


class DiagnoseResponse(BaseModel):
    """è¯Šæ–­å“åº”"""
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    message: str = Field(..., description="è¯Šæ–­ä¿¡æ¯")


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field("ok", description="æœåŠ¡çŠ¶æ€")
    version: str = Field("1.0.0", description="APIç‰ˆæœ¬")


# ============ å…¨å±€çŠ¶æ€ ============

class AppState:
    """åº”ç”¨çŠ¶æ€"""
    def __init__(self):
        self.settings = None
        self.executor = ThreadPoolExecutor(max_workers=4)


app_state = AppState()


# ============ ç”Ÿå‘½å‘¨æœŸç®¡ç† ============

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åŠ è½½é…ç½®
    try:
        app_state.settings = load_settings()
        print("âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        app_state.settings = None
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    app_state.executor.shutdown(wait=True)
    print("âœ“ èµ„æºæ¸…ç†å®Œæˆ")


# ============ FastAPI åº”ç”¨ ============

app = FastAPI(
    title="IPä»£ç†æ± çˆ¬è™« API",
    description="æä¾›ä»£ç†çˆ¬å–ã€æ£€æŸ¥ã€è·å–ç­‰åŠŸèƒ½çš„REST API",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
)


# ============ è¾…åŠ©å‡½æ•° ============

def _check_settings():
    """æ£€æŸ¥é…ç½®æ˜¯å¦å·²åŠ è½½"""
    if app_state.settings is None:
        raise HTTPException(status_code=500, detail="é…ç½®æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")


async def _run_in_thread(func, *args, **kwargs):
    """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œé˜»å¡å‡½æ•°"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(app_state.executor, func, *args, **kwargs)


# ============ API è·¯ç”± ============

@app.get("/", response_model=HealthResponse, tags=["ç³»ç»Ÿ"])
async def root():
    """æ ¹è·¯å¾„ - å¥åº·æ£€æŸ¥"""
    return HealthResponse()


@app.get("/health", response_model=HealthResponse, tags=["ç³»ç»Ÿ"])
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return HealthResponse()


@app.post("/api/v1/crawl-custom", response_model=CrawlCustomResponse, tags=["çˆ¬è™«"])
async def crawl_custom(request: CrawlCustomRequest):
    """
    çˆ¬å–è‡ªå®šä¹‰URLçš„ä»£ç†æ•°æ®
    
    - **url**: ç›®æ ‡ç½‘é¡µURL
    - **max_pages**: æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤1ï¼‰
    - **use_ai**: æ˜¯å¦å¯ç”¨AIè¾…åŠ©è§£æï¼ˆé»˜è®¤falseï¼‰
    - **render_js**: æ˜¯å¦ä½¿ç”¨Playwrightæ¸²æŸ“JSï¼ˆé»˜è®¤falseï¼‰
    - **no_store**: æ˜¯å¦ä¸å­˜å‚¨åˆ°MySQLï¼ˆé»˜è®¤falseï¼‰
    - **verbose**: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤falseï¼‰
    """
    _check_settings()
    
    if not app_state.settings.dynamic_crawler_enabled:
        raise HTTPException(status_code=403, detail="åŠ¨æ€çˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨")
    
    try:
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œçˆ¬å–
        def _crawl():
            return crawl_custom_url(
                settings=app_state.settings,
                url=str(request.url),
                max_pages=request.max_pages or app_state.settings.max_pages,
                use_ai=request.use_ai,
                no_store=request.no_store,
                verbose=request.verbose,
                render_js=request.render_js,
            )
        
        result: DynamicCrawlResult = await _run_in_thread(_crawl)
        
        # è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯
        response_data = {
            "success": result.stored > 0 or result.extracted > 0,
            "url": result.url,
            "session_id": result.session_id,
            "total_ips": result.extracted,
            "stored": result.stored,
            "avg_confidence": 0.0,
            "ai_calls_count": 0,
            "llm_cost_usd": 0.0,
            "review_pending_count": 0,
        }
        
        if result.session_id is not None:
            try:
                crawler = DynamicCrawler(app_state.settings)
                session_stats = crawler.get_session_stats(int(result.session_id))
                response_data["total_ips"] = session_stats.get("ip_count", result.extracted)
                response_data["avg_confidence"] = session_stats.get("avg_extraction_confidence", 0.0)
                response_data["ai_calls_count"] = session_stats.get("llm_calls", 0)
                response_data["llm_cost_usd"] = session_stats.get("llm_cost_usd", 0.0)
                response_data["review_pending_count"] = session_stats.get("review_pending_count", 0)
            except Exception:
                pass
        
        return CrawlCustomResponse(**response_data)
    
    except Exception as e:
        return CrawlCustomResponse(
            success=False,
            url=str(request.url),
            error=str(e)
        )


@app.post("/api/v1/run", response_model=RunCrawlerResponse, tags=["çˆ¬è™«"])
async def run_crawler(request: RunCrawlerRequest, background_tasks: BackgroundTasks):
    """
    è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹ï¼ˆåå°ä»»åŠ¡ï¼‰
    
    - **quick_test**: å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†ç¬¬ä¸€ä¸ªæˆåŠŸçš„æº
    - **quick_record_limit**: å¿«é€Ÿæ¨¡å¼ä¸‹çš„è®°å½•é™åˆ¶
    """
    _check_settings()
    
    def _run():
        run_once(
            app_state.settings,
            quick_test=request.quick_test,
            quick_record_limit=request.quick_record_limit,
        )
    
    background_tasks.add_task(_run)
    
    return RunCrawlerResponse(
        success=True,
        message="çˆ¬è™«ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
    )


@app.post("/api/v1/check", response_model=CheckResponse, tags=["ä»£ç†æ£€æŸ¥"])
async def check_proxies(background_tasks: BackgroundTasks):
    """
    è¿è¡ŒTCPæ‰¹é‡æ£€æŸ¥ï¼ˆåå°ä»»åŠ¡ï¼‰
    
    æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ‰€æœ‰ä»£ç†ï¼Œæ›´æ–°å…¶è¿é€šæ€§å’Œåˆ†æ•°
    """
    _check_settings()
    
    def _check():
        check_pool.run_check_batch(app_state.settings)
    
    background_tasks.add_task(_check)
    
    return CheckResponse(
        success=True,
        message="ä»£ç†æ£€æŸ¥ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨"
    )


@app.get("/api/v1/get-proxy", response_model=GetProxyResponse, tags=["ä»£ç†è·å–"])
async def get_proxies(
    count: int = Query(1, ge=1, le=1000, description="ä»£ç†æ•°é‡"),
    protocol: Optional[str] = Query(None, description="åè®®: http, https, socks4, socks5"),
    country: Optional[str] = Query(None, description="å›½å®¶ä»£ç "),
    min_score: Optional[int] = Query(None, ge=0, le=100, description="æœ€å°åˆ†æ•°"),
):
    """
    ä»ä»£ç†æ± è·å–ä»£ç†
    
    - **count**: è·å–çš„ä»£ç†æ•°é‡ï¼ˆ1-1000ï¼‰
    - **protocol**: è¿‡æ»¤åè®®ç±»å‹
    - **country**: è¿‡æ»¤å›½å®¶ä»£ç ï¼ˆå¦‚ US, CNï¼‰
    - **min_score**: æœ€å°åˆ†æ•°è¦æ±‚ï¼ˆ0-100ï¼‰
    """
    _check_settings()
    
    try:
        # è§£æåè®®å’Œå›½å®¶å‚æ•°
        protocols = [p.strip() for p in protocol.split(",")] if protocol else None
        countries = [c.strip() for c in country.split(",")] if country else None
        
        # åœ¨çº¿ç¨‹æ± ä¸­è·å–ä»£ç†
        def _get():
            return pick_proxies(
                settings=app_state.settings,
                protocols=protocols,
                countries=countries,
                count=count,
                require_check=True,
            )
        
        result = await _run_in_thread(_get)
        
        # å¤„ç†è¿”å›ç»“æœ
        if result.get("status") == "error":
            raise HTTPException(status_code=500, detail=result.get("message", "æœªçŸ¥é”™è¯¯"))
        
        proxies_data = result.get("data", [])
        
        return GetProxyResponse(
            success=True,
            count=len(proxies_data),
            proxies=[
                {
                    "ip": p["ip"],
                    "port": p["port"],
                    "protocol": p["protocol"],
                    "country": p.get("country"),
                    "score": p.get("score", 0),
                    "last_ok": p.get("last_ok"),
                }
                for p in proxies_data
            ]
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»£ç†å¤±è´¥: {str(e)}")


@app.get("/api/v1/diagnose/sources", response_model=DiagnoseResponse, tags=["è¯Šæ–­"])
async def diagnose_sources_endpoint():
    """
    æ£€æŸ¥æ‰€æœ‰åŸå§‹ä»£ç†æºçš„å¯ç”¨æ€§
    
    è¿”å›æ¯ä¸ªæºçš„HTTPçŠ¶æ€å’Œå¯è®¿é—®æ€§
    """
    _check_settings()
    
    try:
        # æ•è·è¯Šæ–­è¾“å‡º
        import io
        import sys
        
        def _diagnose():
            old_stdout = sys.stdout
            sys.stdout = buffer = io.StringIO()
            try:
                diagnose_sources.run()
                return buffer.getvalue()
            finally:
                sys.stdout = old_stdout
        
        output = await _run_in_thread(_diagnose)
        
        return DiagnoseResponse(
            success=True,
            message=output
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯Šæ–­å¤±è´¥: {str(e)}")


@app.get("/api/v1/diagnose/pipeline", response_model=DiagnoseResponse, tags=["è¯Šæ–­"])
async def diagnose_pipeline_endpoint():
    """
    æ£€æŸ¥æ•°æ®ç®¡é“ï¼ˆè·å–å’Œè§£æï¼‰
    
    æµ‹è¯•æ¯ä¸ªæºçš„æ•°æ®è·å–å’Œè§£æèƒ½åŠ›
    """
    _check_settings()
    
    try:
        import io
        import sys
        
        def _diagnose():
            old_stdout = sys.stdout
            sys.stdout = buffer = io.StringIO()
            try:
                diagnose_pipeline.run()
                return buffer.getvalue()
            finally:
                sys.stdout = old_stdout
        
        output = await _run_in_thread(_diagnose)
        
        return DiagnoseResponse(
            success=True,
            message=output
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯Šæ–­å¤±è´¥: {str(e)}")


# ============ é”™è¯¯å¤„ç† ============

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """HTTPå¼‚å¸¸å¤„ç†"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"success": False, "error": exc.detail}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """é€šç”¨å¼‚å¸¸å¤„ç†"""
    return JSONResponse(
        status_code=500,
        content={"success": False, "error": f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(exc)}"}
    )


# ============ æœåŠ¡å™¨å¯åŠ¨å‡½æ•° ============

def start_server(host: str = "0.0.0.0", port: int = 8000, env_path: str | None = None):
    """
    å¯åŠ¨ API æœåŠ¡å™¨
    
    Args:
        host: ç›‘å¬åœ°å€ï¼ˆé»˜è®¤ 0.0.0.0ï¼‰
        port: ç›‘å¬ç«¯å£ï¼ˆé»˜è®¤ 8000ï¼‰
        env_path: .env æ–‡ä»¶è·¯å¾„
    """
    # å¦‚æœæä¾›äº† env_pathï¼Œé‡æ–°åŠ è½½é…ç½®
    if env_path:
        app_state.settings = load_settings(env_path)
    
    print(f"ğŸš€ å¯åŠ¨ IPä»£ç†æ±  API æœåŠ¡å™¨...")
    print(f"ğŸ“¡ ç›‘å¬åœ°å€: http://{host}:{port}")
    print(f"ğŸ“š APIæ–‡æ¡£: http://{host}:{port}/docs")
    print(f"ğŸ“– ReDocæ–‡æ¡£: http://{host}:{port}/redoc")
    print(f"âš™ï¸  é…ç½®æ–‡ä»¶: {env_path or '.env'}")
    print()
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
    )


if __name__ == "__main__":
    start_server()
